<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.0">
<title data-rh="true">Reducing Data Labeling Costs with Uncertainty Sampling | Groundlight</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://code.groundlight.ai/python-sdk/blog/active-learning"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Reducing Data Labeling Costs with Uncertainty Sampling | Groundlight"><meta data-rh="true" name="description" content="How Groundlight uses active learning to train accurate vision models while saving on data labeling costs."><meta data-rh="true" property="og:description" content="How Groundlight uses active learning to train accurate vision models while saving on data labeling costs."><meta data-rh="true" property="og:image" content="https://code.groundlight.ai/python-sdk/assets/images/dog-conf-high-8d9cad195e418d3e0bb824529b8a9dfc.png"><meta data-rh="true" name="twitter:image" content="https://code.groundlight.ai/python-sdk/assets/images/dog-conf-high-8d9cad195e418d3e0bb824529b8a9dfc.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-05-15T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="active learning,uncertainty sampling,deep dive"><link data-rh="true" rel="icon" href="/python-sdk/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://code.groundlight.ai/python-sdk/blog/active-learning"><link data-rh="true" rel="alternate" href="https://code.groundlight.ai/python-sdk/blog/active-learning" hreflang="en"><link data-rh="true" rel="alternate" href="https://code.groundlight.ai/python-sdk/blog/active-learning" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/python-sdk/blog/rss.xml" title="Groundlight RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/python-sdk/blog/atom.xml" title="Groundlight Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-G0XW52NM2K"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-G0XW52NM2K",{})</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/python-sdk/assets/css/styles.89ad1f6a.css">
<script src="/python-sdk/assets/js/runtime~main.c51c14bf.js" defer="defer"></script>
<script src="/python-sdk/assets/js/main.0509e392.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_gu5v" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/python-sdk/"><div class="navbar__logo"><img src="/python-sdk/img/favicon-32x32.png" alt="Groundlight Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/python-sdk/img/favicon-32x32.png" alt="Groundlight Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Groundlight</b></a><a class="navbar__item navbar__link" href="/python-sdk/docs/getting-started">Docs</a><a class="navbar__item navbar__link" href="/python-sdk/docs/building-applications">Applications</a><a href="/python-sdk/api-reference-docs/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API Reference<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/python-sdk/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/groundlight/python-sdk" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_kWbt colorModeToggle_GwZs"><button class="clean-btn toggleButton_fOL9 toggleButtonDisabled_STpu" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_DCeJ"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_DFgp"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_IP3a"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_IbdI"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_GnOS thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_aARK margin-bottom--md">Recent posts</div><ul class="sidebarItemList_a8Ne clean-list"><li class="sidebarItem_Otbb"><a aria-current="page" class="sidebarItemLink_OBo2 sidebarItemLinkActive_guiV" href="/python-sdk/blog/active-learning">Reducing Data Labeling Costs with Uncertainty Sampling</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/python-sdk/blog/groundlight-ai-achieves-soc-2-type-2-compliance">Groundlight AI Achieves SOC 2 Type 2 Compliance</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/python-sdk/blog/dealing-with-unclear-images">Navigating Ambiguity with Groundlight AI Detectors</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/python-sdk/blog/getting-started">Building your first computer vision model just got easier</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/python-sdk/blog/grime-guardian">The Grime Guardian: Building Stateful Multi-camera applications with Groundlight</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="How Groundlight uses active learning to train accurate vision models while saving on data labeling costs."><link itemprop="image" href="https://code.groundlight.ai/python-sdk/assets/images/dog-conf-high-8d9cad195e418d3e0bb824529b8a9dfc.png"><header><h1 class="title_xzwX" itemprop="headline">Reducing Data Labeling Costs with Uncertainty Sampling</h1><div class="container_HY9_ margin-vert--md"><time datetime="2024-05-15T00:00:00.000Z" itemprop="datePublished">May 15, 2024</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_uEq3"><div class="avatar margin-bottom--sm"><img class="avatar__photo" src="https://a-us.storyblok.com/f/1015187/1000x1000/efc35da152/sandlert.jpg" alt="Ted Sandler" itemprop="image"><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><span itemprop="name">Ted Sandler</span></div><small class="avatar__subtitle" itemprop="description">Senior Applied Scientist</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p>At Groundlight, we train each detector&#x27;s machine learning (ML) model on images that have been manually labeled with correct responses. However, collecting labels at scale becomes expensive because it requires human review. Given that detectors are frequently applied to streams of images that change slowly over time, reviewing all images as they arrive is likely to result in effort wasted on labeling similar images that add little information to the training set.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="what-is-active-learning-in-machine-learning">What is Active Learning in Machine Learning?<a href="#what-is-active-learning-in-machine-learning" class="hash-link" aria-label="Direct link to What is Active Learning in Machine Learning?" title="Direct link to What is Active Learning in Machine Learning?">​</a></h2>
<p>To avoid unnecessary labeling and save customers money, Groundlight uses <strong><a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" target="_blank" rel="noopener noreferrer">active learning</a></strong>, a machine learning protocol in which the ML model plays an active role in determining which images get manually labeled for training. With active learning, only informative images are prioritized for review, making it possible to label small a subset of the available data but train a model that&#x27;s roughly as good as one trained with all the data labeled <a href="https://minds.wisconsin.edu/handle/1793/60660" target="_blank" rel="noopener noreferrer">[Settles, 2009]</a>.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="what-is-uncertainty-sampling">What is Uncertainty Sampling?<a href="#what-is-uncertainty-sampling" class="hash-link" aria-label="Direct link to What is Uncertainty Sampling?" title="Direct link to What is Uncertainty Sampling?">​</a></h2>
<p>The variant of active learning we use at Groundlight is based on <strong><a href="https://lilianweng.github.io/posts/2022-02-20-active-learning/#uncertainty-sampling" target="_blank" rel="noopener noreferrer">uncertainty sampling</a></strong>, a well studied and effective method that can be used in either the streaming setting or the pool-based setting in which there exists a large reservoir of unlabeled examples to draw from. We operate in the stream-based setting, where images arrive one at a time and it must be decided in the moment whether to escalate an image for review.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="how-does-uncertainty-sampling-work">How Does Uncertainty Sampling Work?<a href="#how-does-uncertainty-sampling-work" class="hash-link" aria-label="Direct link to How Does Uncertainty Sampling Work?" title="Direct link to How Does Uncertainty Sampling Work?">​</a></h2>
<p>Imagine we have a detector that processes a stream of images arriving one by one. The detector&#x27;s ML model is trained on all images labeled up to that point in time. When a new image arrives, the model makes its best guess prediction for the new image and also reports its confidence in that prediction. The confidence is expressed as a probability (a number between zero and one) that the prediction is correct.</p>
<p>In uncertainty sampling, we escalate those images whose predictions have low confidence so they can be manually reviewed and labeled. Conversely, we largely leave images with confident predictions unescalated and therefore unlabeled. In this way, we avoid the expense and effort of labeling images whose predictions are likely correct. But we still continue to label images the model is unsure of so it can improve on them.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="an-example-of-uncertainty-sampling">An Example of Uncertainty Sampling<a href="#an-example-of-uncertainty-sampling" class="hash-link" aria-label="Direct link to An Example of Uncertainty Sampling" title="Direct link to An Example of Uncertainty Sampling">​</a></h2>
<p>As an example, the images shown below were sent to a detector that identifies the presence of dogs in and around a swimming pool at <a href="https://dogmode.com/aquatic-fitness-center-pool-view/" target="_blank" rel="noopener noreferrer">Dogmode&#x27;s Aquatic Center</a>. The model reports with 95% confidence that there is a dog in the image on the left. But it is less confident in its response for the image on the right, saying there is no dog present with only 75% confidence. (There is in fact a dog at the back left corner of the pool, but it’s difficult to see.)</p>
<table cellspacing="0" cellpadding="0"><center><tr><td><img src="/python-sdk/assets/images/dog-conf-high-8d9cad195e418d3e0bb824529b8a9dfc.png"></td><td><img src="/python-sdk/assets/images/dog-conf-low-bc13114624d9f5ee2d69d325ffda59b7.png"></td></tr><tr><td>Yes</td><td>No</td></tr></center></table>
<br>
<p>Assuming the detector&#x27;s confidence threshold is set to a value between 75% and 95%, uncertainty sampling will escalate the image on the right for cloud labeling but not the one on the left. A user can set their detector&#x27;s confidence threshold by adjusting the confidence threshold slider on the detector detail page. The image below shows this slider.</p>
<img src="/python-sdk/assets/images/confidence-threshold-89f6ddecf93d7defede23cc6f810a190.png">
<br>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="experiment-with-uncertainty-sampling">Experiment with Uncertainty Sampling<a href="#experiment-with-uncertainty-sampling" class="hash-link" aria-label="Direct link to Experiment with Uncertainty Sampling" title="Direct link to Experiment with Uncertainty Sampling">​</a></h2>
<p>We now present results from a time-series experiment on images collected and labeled for the purpose of measuring uncertainty sampling&#x27;s impact on model accuracy and labeling cost. There are 500 images of a gate, and the task is to determine in every image if the gate has been left open or closed. All images in the experiment are labeled so we know the correct responses. But note<sup><a href="#user-content-fn-1-f95320" id="user-content-fnref-1-f95320" data-footnote-ref="true" aria-describedby="footnote-label">1</a></sup> that this would not be the case if we were running active learning in real life because, by design, active learning does not recruit labels on high confidence images.</p>
<p>Our results compare the performance of three models trained under different protocols:</p>
<ol>
<li>No uncertainty sampling, all images are escalated for manual review and labeling</li>
<li>Moderate uncertainty sampling, predictions less than 95% confident get escalated</li>
<li>Aggressive uncertainty sampling, only predictions below 75% confidence are escalated</li>
</ol>
<p>The training sets of all three models are initialized with the same 20 images, 10 labeled from each class.</p>
<p>The plot below shows that the model trained with moderate uncertainty sampling (confidence threshold 95%) has an error rate similar to the model trained without any uncertainty sampling. This demonstrates that uncertainty sampling can fit a model as accurately as labeling and training on all the available data.</p>
<img src="/python-sdk/assets/images/error-rate-over-time-5a8a630ef2e86617224ed794f5b4c583.png" width="500 px">
<p>On the other hand, aggressive uncertainty sampling (confidence threshold 75%) escalates too few images for labeling, resulting in a model trained on less data which makes more mistakes. This shows how the confidence threshold controls the trade off between model accuracy and labeling costs from manual labeling. Indirectly, it also demonstrates the need for calibrating models so their reported confidences reflect observed frequencies and can be used for making decisions about when to escalate. We calibrate machine learning models at Groundlight though the details are beyond the scope of this post.</p>
<p>Strikingly, plotting the number of images escalated by each model shows that active learning dramatically reduces labeling costs. The model trained without uncertainty sampling escalates all 500 images for review manual review. In contrast, the model trained with moderate uncertainty sampling escalates only 132 images in total. This is a nearly 75% reduction in manual labeling and cost with little change in model error. Aggressive uncertainty sampling escalates even fewer images, only 60, but the resulting model has noticeably higher error as observed in the plot above.</p>
<img src="/python-sdk/assets/images/labels-over-time-562d3b801755d00733fad600f1a38443.png" width="500 px">
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2>
<p>At Groundlight, we use active learning to reduce labeling costs for our customers. In particular, we use a variant based on uncertainty sampling that is extremely effective and easy to explain. A small experiment on an image time-series dataset shows that uncertainty sampling can dramatically reduce the number of images labeled without impacting model accuracy. If you want to learn more about active learning and its various formulations, definitely check out the references below.</p>
<h2 class="anchor anchorWithStickyNavbar_FNw8" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2>
<ol>
<li><a href="https://minds.wisconsin.edu/handle/1793/60660" target="_blank" rel="noopener noreferrer">Settles, Burr. <em>Active learning literature survey</em>. University of Wisconsin-Madison Department of Computer Sciences, 2009.</a></li>
<li><a href="https://lilianweng.github.io/posts/2022-02-20-active-learning/" target="_blank" rel="noopener noreferrer">Weng, Lilian. &quot;Learning with not Enough Data Part 2: Active Learning.&quot; Lil&#x27;Log, February 20 2022. April 29 2024.</a></li>
</ol>
<section data-footnotes="true" class="footnotes"><h2 class="anchor anchorWithStickyNavbar_FNw8 sr-only" id="footnote-label">Footnotes<a href="#footnote-label" class="hash-link" aria-label="Direct link to Footnotes" title="Direct link to Footnotes">​</a></h2>
<ol>
<li id="user-content-fn-1-f95320">
<p>In practice, we audit a constant fraction of confidently predicted images for review that serve as an additional source of labeled data. <a href="#user-content-fnref-1-f95320" data-footnote-backref="" aria-label="Back to reference 1" class="data-footnote-backref">↩</a></p>
</li>
</ol>
</section></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_vccZ"><div class="col"><b>Tags:</b><ul class="tags_aHIs padding--none margin-left--sm"><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/python-sdk/blog/tags/active-learning">active learning</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/python-sdk/blog/tags/uncertainty-sampling">uncertainty sampling</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/python-sdk/blog/tags/deep-dive">deep dive</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2024-05-15-active-learning.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_NulP" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/python-sdk/blog/groundlight-ai-achieves-soc-2-type-2-compliance"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Groundlight AI Achieves SOC 2 Type 2 Compliance</div></a></nav></main><div class="col col--2"><div class="tableOfContents_IS5x thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-active-learning-in-machine-learning" class="table-of-contents__link toc-highlight">What is Active Learning in Machine Learning?</a></li><li><a href="#what-is-uncertainty-sampling" class="table-of-contents__link toc-highlight">What is Uncertainty Sampling?</a></li><li><a href="#how-does-uncertainty-sampling-work" class="table-of-contents__link toc-highlight">How Does Uncertainty Sampling Work?</a></li><li><a href="#an-example-of-uncertainty-sampling" class="table-of-contents__link toc-highlight">An Example of Uncertainty Sampling</a></li><li><a href="#experiment-with-uncertainty-sampling" class="table-of-contents__link toc-highlight">Experiment with Uncertainty Sampling</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Documentation</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/python-sdk/docs/getting-started">Getting Started</a></li><li class="footer__item"><a class="footer__link-item" href="/python-sdk/docs/building-applications">Building Applications</a></li><li class="footer__item"><a class="footer__link-item" href="/python-sdk/docs/installation">Installation</a></li></ul></div><div class="col footer__col"><div class="footer__title">Company</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.groundlight.ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">About<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.groundlight.ai/team" target="_blank" rel="noopener noreferrer" class="footer__link-item">Team<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.groundlight.ai/careers" target="_blank" rel="noopener noreferrer" class="footer__link-item">Careers<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://app.groundlight.ai/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sign In<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Code</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/groundlight/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Github<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://pypi.org/project/groundlight/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Python SDK<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/groundlight/stream" target="_blank" rel="noopener noreferrer" class="footer__link-item">Video streaming<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/groundlight/esp32cam" target="_blank" rel="noopener noreferrer" class="footer__link-item">Arduino<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Groundlight AI.</div></div></div></footer></div>
</body>
</html>
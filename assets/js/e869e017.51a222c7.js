"use strict";(self.webpackChunkcode_groundlight_ai=self.webpackChunkcode_groundlight_ai||[]).push([[6208],{7474:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>o,default:()=>l,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"guide/grabbing-images","title":"Grabbing Images","description":"In order to analyze images with Groundlight, you first need to capture images from a camera or other image source. This guide will show you how to capture images from different sources and formats.","source":"@site/docs/guide/2-grabbing-images.md","sourceDirName":"guide","slug":"/guide/grabbing-images","permalink":"/python-sdk/docs/guide/grabbing-images","draft":false,"unlisted":false,"editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/docs/guide/2-grabbing-images.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Guide","permalink":"/python-sdk/docs/guide/"},"next":{"title":"Working with Detectors","permalink":"/python-sdk/docs/guide/working-with-detectors"}}');var i=n(4848),t=n(8453);const s={},o="Grabbing Images",c={},d=[{value:"Framegrab",id:"framegrab",level:2},{value:"Capturing Images",id:"capturing-images",level:3},{value:"Motion Detection",id:"motion-detection",level:3},{value:"Image Formats",id:"image-formats",level:2},{value:"PIL",id:"pil",level:3},{value:"OpenCV",id:"opencv",level:3},{value:"Numpy",id:"numpy",level:3},{value:"Channel order: BGR vs RGB",id:"channel-order-bgr-vs-rgb",level:4}];function h(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"grabbing-images",children:"Grabbing Images"})}),"\n",(0,i.jsx)(r.p,{children:"In order to analyze images with Groundlight, you first need to capture images from a camera or other image source. This guide will show you how to capture images from different sources and formats."}),"\n",(0,i.jsx)(r.h2,{id:"framegrab",children:"Framegrab"}),"\n",(0,i.jsxs)(r.p,{children:["For a unified interface to many different kinds of image sources, see ",(0,i.jsx)(r.a,{href:"https://pypi.org/project/framegrab/",children:"framegrab"}),", an ",(0,i.jsx)(r.a,{href:"https://github.com/groundlight/framegrab",children:"open-source"})," python library maintained by Groundlight."]}),"\n",(0,i.jsx)(r.h3,{id:"capturing-images",children:"Capturing Images"}),"\n",(0,i.jsx)(r.p,{children:"Framegrab has many useful features for working with cameras and other image sources. It provides a single interface for extracting images from many different image sources, including generic USB cameras (such as webcams), RTSP streams, HTTP live streams, YouTube live streams, Basler USB cameras, Basler GigE cameras, and Intel RealSense depth cameras."}),"\n",(0,i.jsx)(r.p,{children:"Installation is straightforward:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"pip install framegrab[all]\n"})}),"\n",(0,i.jsxs)(r.p,{children:["To capture frames, first configure a ",(0,i.jsx)(r.code,{children:"FrameGrabber"})," object, specifying the image source. Then call the ",(0,i.jsx)(r.code,{children:"grab()"})," method to capture a frame:"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",metastring:"notest",children:"from framegrab import FrameGrabber\n\n# Create a FrameGrabber for a generic USB camera (e.g., a webcam)\nconfig = {\n    'input_type': 'generic_usb',\n}\ngrabber = FrameGrabber.create_grabber(config)\n\nframe = grabber.grab()\n"})}),"\n",(0,i.jsx)(r.p,{children:"Framegrab returns images as numpy arrays in BGR format, which is the standard format for OpenCV. This makes it easy to use the images with other image processing libraries, such as OpenCV."}),"\n",(0,i.jsxs)(r.p,{children:["See the ",(0,i.jsx)(r.a,{href:"https://github.com/groundlight/framegrab/blob/main/README.md",children:"framegrab documentation"})," for more information on configuring different image sources."]}),"\n",(0,i.jsx)(r.h3,{id:"motion-detection",children:"Motion Detection"}),"\n",(0,i.jsx)(r.p,{children:"Framegrab also includes a motion detection module, which can be used to detect motion in a video stream. This can be useful for detecting when something changes in a scene, such as when a person enters a room or a car pulls into a parking space."}),"\n",(0,i.jsxs)(r.p,{children:["To use the built-in motion detection functionality, first create a ",(0,i.jsx)(r.code,{children:"MotionDetector"})," object, specifying the percentage threshold for motion detection. Then, use the motion_detected() method with every captured frame to check if motion has been detected:"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",metastring:"notest",children:"from framegrab import FrameGrabber, MotionDetector\n\nconfig = {'input_type': 'generic_usb'}\ngrabber = FrameGrabber.create_grabber(config)\n\nmotion_threshold = 1.0\nmdet = MotionDetector(pct_threshold=motion_threshold)\n\nwhile True:\n    frame = grabber.grab()\n    if frame is None:\n        print(\"No frame captured!\")\n        continue\n\n    if mdet.motion_detected(frame):\n        print(\"Motion detected!\")\n"})}),"\n",(0,i.jsxs)(r.p,{children:["In this example, ",(0,i.jsx)(r.code,{children:"motion_threshold"})," specifies the sensitivity level for detecting motion based on the percentage of pixels that have changed. By default, this is set to 1.0, indicating a 1% change. To increase the sensitivity, set the threshold to a lower value, such as 0.5%. Likewise, to decrease the sensitivity, set the threshold to a higher value, such as 2%."]}),"\n",(0,i.jsx)(r.h2,{id:"image-formats",children:"Image Formats"}),"\n",(0,i.jsx)(r.p,{children:"Groundlight's SDK accepts images in many popular formats, including PIL, OpenCV, and numpy arrays."}),"\n",(0,i.jsx)(r.h3,{id:"pil",children:"PIL"}),"\n",(0,i.jsxs)(r.p,{children:["The Groundlight SDK can accept PIL images directly in ",(0,i.jsx)(r.code,{children:"submit_image_query"}),".  Here's an example:"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from groundlight import Groundlight\nfrom PIL import Image\n\ngl = Groundlight()\ndet = gl.get_or_create_detector(name="path-clear", query="Is the path clear?")\npil_img = Image.open("./docs/static/img/doorway.jpg")\ngl.submit_image_query(det, pil_img)\n'})}),"\n",(0,i.jsx)(r.h3,{id:"opencv",children:"OpenCV"}),"\n",(0,i.jsxs)(r.p,{children:["OpenCV is a popular image processing library, with many utilities for working with images.\nOpenCV images are stored as numpy arrays.  (Note they are stored in BGR order, not RGB order, but as of Groundlight SDK v0.8 this is the expected order.)\nOpenCV's images can be send directly to ",(0,i.jsx)(r.code,{children:"submit_image_query"})," as follows:"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",metastring:"notest",children:"import cv2\n\ncam = cv2.VideoCapture(0)  # Initialize camera (0 is the default index)\n\n_, frame = cam.read()  # Capture one frame\ngl.submit_image_query(detector, frame)  # Send the frame to Groundlight\ncam.release()  # Release the camera\n"})}),"\n",(0,i.jsx)(r.h3,{id:"numpy",children:"Numpy"}),"\n",(0,i.jsxs)(r.p,{children:["The Groundlight SDK can accept images as ",(0,i.jsx)(r.code,{children:"numpy"})," arrays. They should be in the standard HWN format in BGR color order, matching OpenCV standards.\nPixel values should be from 0-255 (not 0.0-1.0 as floats). So ",(0,i.jsx)(r.code,{children:"uint8"})," data type is preferable since it saves memory."]}),"\n",(0,i.jsx)(r.p,{children:"Here's sample code to create an 800x600 random image in numpy:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",metastring:"notest",children:"import numpy as np\n\nnp_img = np.random.uniform(low=0, high=255, size=(600, 800, 3)).astype(np.uint8)\n# Note: channel order is interpretted as BGR not RGB\ngl.submit_image_query(detector, np_img)\n"})}),"\n",(0,i.jsx)(r.h4,{id:"channel-order-bgr-vs-rgb",children:"Channel order: BGR vs RGB"}),"\n",(0,i.jsx)(r.p,{children:"Groundlight expects images in BGR order, because this is standard for OpenCV, which uses numpy arrays as image storage.\n(OpenCV uses BGR because it was originally developed decades ago for compatibility with the BGR color format used by many cameras and image processing hardware at the time of its creation.)\nMost other image libraries use RGB order, so if you are using images as numpy arrays which did not originate from OpenCV you likely need to reverse the channel order before sending the images to Groundlight.\nNote this change was made in v0.8 of the Groundlight SDK - in previous versions, RGB order was expected."}),"\n",(0,i.jsx)(r.p,{children:"If you have an RGB array, you must reverse the channel order before sending it to Groundlight, like:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",metastring:"notest",children:"# Convert numpy image in RGB channel order to BGR order\nbgr_img = rgb_img[:, :, ::-1]\n"})}),"\n",(0,i.jsx)(r.p,{children:"The difference can be surprisingly subtle when red and blue get swapped.  Often images just look a little off, but sometimes they look very wrong."}),"\n",(0,i.jsxs)(r.p,{children:["Here's an example of a natural-scene image where you might think the color balance is just off:\n",(0,i.jsx)(r.img,{alt:"Correct color order",src:n(7140).A+"",width:"800",height:"600"}),"\n",(0,i.jsx)(r.img,{alt:"Swapped color channels",src:n(7722).A+"",width:"800",height:"600"})]}),"\n",(0,i.jsxs)(r.p,{children:["In industrial settings, the difference can be almost impossible to detect without prior knowledge of the scene:\n",(0,i.jsx)(r.img,{alt:"Correct color order",src:n(6183).A+"",width:"1418",height:"979"}),"\n",(0,i.jsx)(r.img,{alt:"Swapped color channels",src:n(7261).A+"",width:"1418",height:"979"})]})]})}function l(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},7261:(e,r,n)=>{n.d(r,{A:()=>a});const a=n.p+"assets/images/cnc-gripper-bgr-3f62a203a84000db8dffb8b542aa8706.jpg"},6183:(e,r,n)=>{n.d(r,{A:()=>a});const a=n.p+"assets/images/cnc-gripper-4e667290978a62db9edcb5859e42da1b.jpg"},7722:(e,r,n)=>{n.d(r,{A:()=>a});const a=n.p+"assets/images/michonne-bgr-b7e5ce1b0ea7a8fad1386bb13f0b13cb.jpg"},7140:(e,r,n)=>{n.d(r,{A:()=>a});const a=n.p+"assets/images/michonne-d62a9f110b7dab2a04b1dca75c5cf39c.jpg"},8453:(e,r,n)=>{n.d(r,{R:()=>s,x:()=>o});var a=n(6540);const i={},t=a.createContext(i);function s(e){const r=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(t.Provider,{value:r},e.children)}}}]);
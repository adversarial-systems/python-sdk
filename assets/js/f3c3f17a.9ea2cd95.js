"use strict";(self.webpackChunkweb=self.webpackChunkweb||[]).push([[4935],{1625:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"groundlight-ai-achieves-soc-2-type-2-compliance","metadata":{"permalink":"/python-sdk/blog/groundlight-ai-achieves-soc-2-type-2-compliance","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2024-05-13-soc2-announcement.md","source":"@site/blog/2024-05-13-soc2-announcement.md","title":"Groundlight AI Achieves SOC 2 Type 2 Compliance","description":"Groundlight AI Achieves SOC 2 Type 2 Compliance","date":"2024-05-13T00:00:00.000Z","formattedDate":"May 13, 2024","tags":[{"label":"soc-2","permalink":"/python-sdk/blog/tags/soc-2"}],"readingTime":2.645,"hasTruncateMarker":true,"authors":[{"name":"Phillipie Motley","title":"Operations Manager","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/1902e83e56/motleyp.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/1902e83e56/motleyp.jpg"}],"frontMatter":{"title":"Groundlight AI Achieves SOC 2 Type 2 Compliance","description":"Groundlight AI Achieves SOC 2 Type 2 Compliance","slug":"groundlight-ai-achieves-soc-2-type-2-compliance","authors":[{"name":"Phillipie Motley","title":"Operations Manager","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/1902e83e56/motleyp.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/1902e83e56/motleyp.jpg"}],"tags":["soc-2"],"hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Navigating Ambiguity with Groundlight AI Detectors","permalink":"/python-sdk/blog/dealing-with-unclear-images"}},"content":"At Groundlight, we take data security and privacy extremely seriously. From the very beginning, we recognized the importance of implementing stringent controls and processes to safeguard our clients\' sensitive information. That\'s why we made the decision early on to pursue SOC 2 compliance.\\n\\n## What is SOC 2 Verification\\n\\nFor those unfamiliar, SOC 2 (Service Organization Control 2) is an auditing framework established by the American Institute of Certified Public Accountants (AICPA). It involves an in-depth external review of an organization\'s security policies, procedures, and controls by an independent auditor. Achieving SOC 2 certification demonstrates our unwavering commitment to maintaining the highest standards of data protection and privacy.\\n\\n### What Are the Different SOC 2 Types?\\n\\n**SOC 2 Type 1:** Evaluates an organization\'s cybersecurity controls at a single point in time.\\n\\n**SOC 2 Type 2:**  Type 2 report assesses the operational effectiveness of controls over a defined period of time (3, 6, 12 months).\\n\\n## How Did Groundlight Achieve SOC 2 Compliance\\n\\nAchieving SOC 2 compliance is a marathon, not a sprint. It demands meticulous planning and dedication from teams across the entire organization. At Groundlight, we took a methodical approach by first establishing an audit timeline. From there, we worked backwards systematically to get our house in order.\\nTeams across engineering, security, operations, and more collaborated to implement rigorous security policies and controls. We overhauled processes for everything from access management to incident response handling. Robust evidence collection and documentation mechanisms were put into place.\\nOnce we had thoroughly prepared, we brought in external auditors to conduct their independent evaluation. This was the high-stakes final exam. Our policies, technical safeguards, and control operations were stress-tested and scrutinized over an extended period.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What Does SOC 2 Verification Mean for Groundlight AI\u2019s Data Security\\n\\nFrom day one, Groundlight has made data security and privacy a top priority. Safeguarding our customers\' sensitive information is foundational to our business. So while achieving SOC 2 certification marks an important milestone, it simply reinforces practices that have been ingrained in our DNA all along.\\nWe\'ve never treated security as an afterthought or box to check. Instead, we\'ve embraced building robust data protections into the core of our products and services from the ground up. Our policies, processes and technical controls are meticulously tailored to our unique operations - not generic one-size-fits-all measures.\\nSOC 2 compliance validates that we\'ve institutionalized this security-first mindset across the entire organization. But it\'s just one step along our continuous journey. As data privacy regulations evolve and new threats emerge, we\'ll remain vigilant in regularly reassessing and elevating our safeguards.\\nBy upholding the highest standards like SOC 2, we solidify the unshakable foundation of trust with our customers. Upholding these compliance standards unlocks new business opportunities and allows us to double down on our commitment to being steadfast in data security.\\n\\n## Key Takeaways of SOC 2 Verification\\n\\n- Prioritizing security and tailoring controls to our needs, not just checking boxes\\n- SOC 2 enables new business growth by meeting vendor security requirements \\n- Earning certification required full organizational commitment and stakeholder participation\\n- This marks an important milestone, but our security journey is never complete"},{"id":"dealing-with-unclear-images","metadata":{"permalink":"/python-sdk/blog/dealing-with-unclear-images","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2024-03-20-unclear-blog.md","source":"@site/blog/2024-03-20-unclear-blog.md","title":"Navigating Ambiguity with Groundlight AI Detectors","description":"Let\'s talk more about ambiguous image queries","date":"2024-03-20T00:00:00.000Z","formattedDate":"March 20, 2024","tags":[{"label":"unclears","permalink":"/python-sdk/blog/tags/unclears"},{"label":"real-world ambiguity","permalink":"/python-sdk/blog/tags/real-world-ambiguity"}],"readingTime":3.155,"hasTruncateMarker":true,"authors":[{"name":"Sharmila Reddy Nangi","title":"Applied ML Scientist","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/b66d1cddeb/nangis.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/b66d1cddeb/nangis.jpg"}],"frontMatter":{"title":"Navigating Ambiguity with Groundlight AI Detectors","description":"Let\'s talk more about ambiguous image queries","slug":"dealing-with-unclear-images","authors":[{"name":"Sharmila Reddy Nangi","title":"Applied ML Scientist","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/b66d1cddeb/nangis.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/b66d1cddeb/nangis.jpg"}],"tags":["unclears","real-world ambiguity"],"image":"./images/unclear_blog/unclear_label.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Groundlight AI Achieves SOC 2 Type 2 Compliance","permalink":"/python-sdk/blog/groundlight-ai-achieves-soc-2-type-2-compliance"},"nextItem":{"title":"Building your first computer vision model just got easier","permalink":"/python-sdk/blog/getting-started"}},"content":"When you first explore the capabilities of our Groundlight AI detectors, you\'ll quickly notice that they excel at answering binary questions. These are queries expecting a straightforward \\"Yes\\" or \\"No\\" response. However, the world around us rarely conforms to such black-and-white distinctions, particularly when analyzing images. In reality, many scenarios present challenges that defy a simple binary answer.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Exploring the Gray Areas: Real-World Examples\\n\\nConsider the following scenarios that highlight the complexity of interpreting real-world images:\\n\\n1. **The Case of the Hidden Oven**: Imagine asking, \\"Is the oven light turned on?\\" only to find the view partially blocked by a person. With the contents on the other side hidden from view, providing a definitive \\"Yes\\" or \\"No\\" becomes impossible. Such instances are best described as \\"Unclear.\\"\\n<figure style={{ textAlign: \'center\' }}>\\n    <img src={require(\'./images/unclear_blog/hidden_oven.png\').default} width=\\"350px\\"/>\\n    <figcaption>\\n    <small>\\n     Oven is hidden from the camera view\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n2. **The Locked Garage Door Dilemma**: When faced with a query like, \\"Is the garage door locked?\\" accompanied by an image shrouded in darkness or blurred beyond recognition, identifying the status of the door lock is a challenge. In these circumstances, clarity eludes us, leaving us unable to confidently answer.\\n<figure style={{ textAlign: \'center\' }}>\\n    <img src={require(\'./images/unclear_blog/dark_door.png\').default} width=\\"350px\\"/>\\n    <figcaption>\\n    <small>\\n     Dark images make it difficult to answer the query\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n3. **Irrelevant Imagery**: At times, the images presented may bear no relation to the question posed. These irrelevant scenes further underscore the limitations of binary responses in complex situations. For instance, responding to the question \\"Is there a black jacket on the coat hanger?\\" with the following image (that doesn\'t even include a coat hanger) exemplifies how such imagery can be off-topic and fail to address the query appropriately.\\n<figure style={{ textAlign: \'center\' }}>\\n    <img src={require(\'./images/unclear_blog/unrelated_img.png\').default} width=\\"350px\\" />\\n    <figcaption>\\n    <small>\\n    Images unrelated to the query lead to ambiguity\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n\\n## Strategies for Navigating Ambiguity\\n\\nAlthough encountering unclear images might seem like a setback, it actually opens up avenues for improvement and customization. Our detectors are designed to identify and flag these ambiguous cases, empowering you to steer their interpretation. Here are some strategies you can employ to enhance the process:\\n\\n1. **Clarify your queries** : It\'s crucial to formulate your questions to the system with precision, avoiding any vagueness. For instance, instead of asking, \u201cIs the light ON?\u201d opt for a more detailed inquiry such as, \u201cCan you clearly see the red LED on the right panel turned ON?\u201d This approach ensures your queries are direct and specific.\\n2. **Customize Yes/ No classifications**: You can specify how the model should interpret and deal with unclear images by reframing your queries and notes. For instance, by specifying \u201cIf the garage door is not visible, mark it as a NO\u201d in your notes, you can make the detector sort unclear images into the \u201cNO\u201d class. You can refer to our [previous blog post](https://code.groundlight.ai/python-sdk/blog/best-practices) for best practices while refining your queries and notes.\\n3. **Flagging \u201cUnclear\u201d images**: Should you prefer to classify an obstructed view or irrelevant imagery as \u201cUnclear\u201d, simply add a couple of labels as \u201cUNCLEAR\u201d or provide instructions in the notes. Groundlight\'s machine learning systems will adapt to your preference and continue to flag them as \\"Unclear\\" for you.\\n<figure style={{ textAlign: \'center\' }}>\\n    <img src={require(\'./images/unclear_blog/unclear_label.png\').default} width=\\"350px\\" class=\\"center\\"/>\\n    <figcaption>\\n    <small>\\n    Marking an image query as \u201cUnclear\\" in the data review page\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n\\nThe strategies outlined above will significantly improve your ability to navigate through unclear \\nscenarios. However, there exist many other situations, such as borderline classifications or cases where there\'s insufficient information for a definitive answer. Recognizing and managing the inherent uncertainty in these tasks is crucial as we progress. We are committed to building more tools that empower you to deal with these challenges."},{"id":"getting-started","metadata":{"permalink":"/python-sdk/blog/getting-started","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2024-02-15-getting_started.md","source":"@site/blog/2024-02-15-getting_started.md","title":"Building your first computer vision model just got easier","description":"We\'re thrilled to announce a new repository that makes it incredibly easy for anyone to get started for free with Groundlight, a computer vision (CV) platform powered by natural language. This first steps guide is designed to walk you through setting up your first Groundlight detector to answer a simple question: \\"Is the door open?\\" Behind the scenes, Groundlight will automatically train and deploy a computer vision model that can answer this question in real time. With our escalation technology, you don\'t need to provide any labeled data - you get answers from your first image submission.","date":"2024-02-15T00:00:00.000Z","formattedDate":"February 15, 2024","tags":[{"label":"Getting started","permalink":"/python-sdk/blog/tags/getting-started"},{"label":"Groundlight Python SDK","permalink":"/python-sdk/blog/tags/groundlight-python-sdk"}],"readingTime":2.925,"hasTruncateMarker":true,"authors":[{"name":"Sunil Kumar","title":"Machine Learning Engineer","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg"}],"frontMatter":{"title":"Building your first computer vision model just got easier","slug":"getting-started","authors":[{"name":"Sunil Kumar","title":"Machine Learning Engineer","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg"}],"tags":["Getting started","Groundlight Python SDK"],"image":"./images/getting_started/cvme1.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Navigating Ambiguity with Groundlight AI Detectors","permalink":"/python-sdk/blog/dealing-with-unclear-images"},"nextItem":{"title":"The Grime Guardian: Building Stateful Multi-camera applications with Groundlight","permalink":"/python-sdk/blog/grime-guardian"}},"content":"We\'re thrilled to announce a new [repository](https://github.com/groundlight/getting_started) that makes it incredibly easy for anyone to get started for free with Groundlight, a computer vision (CV) platform powered by natural language. This first steps guide is designed to walk you through setting up your first Groundlight detector to answer a simple question: \\"Is the door open?\\" Behind the scenes, Groundlight will automatically train and deploy a computer vision model that can answer this question in real time. With our escalation technology, you don\'t need to provide any labeled data - you get answers from your first image submission.\\n\\n[![groundlight/getting_started - GitHub](https://gh-card.dev/repos/groundlight/getting_started.svg)](https://github.com/groundlight/getting_started)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What is Groundlight?\\nGroundlight offers a truly modern take on computer vision, combining the best AI models with real-time human supervision in the cloud. Our Escalation Technology automatically chooses the best solution for your problem - whether that\'s a traditional CV model like an [EfficentNet](https://pytorch.org/hub/nvidia_deeplearningexamples_efficientnet/) on the edge, a powerful Visual LLM in the cloud, or a live sensible human monitor. The result is fairly incredible if you\'re used to the traditional `[\\"gather data\\", \\"train model\\", \\"evaluate\\", \\"repeat\\"]` pattern of machine learning.  Instead, Groundlight empowers you to just phrase the visual question you want answered in English, send in images, and Groundlight provides confidence-calibrated answers.  At first, the answers will be slow and/or unconfident, but after not very many examples you\'re using a customized CV model trained just for your task.\\n\\n<figure>\\n    <img src={require(\'./images/getting_started/escalation_diagram.jpg\').default} />\\n    <figcaption>\\n    <small>\\n     Groundlight\'s escalation technology backs every question you ask Groundlight. Escalation ensures we find the best answer for your question, every time.\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n## What\'s Inside?\\nOur [getting started repository](https://github.com/groundlight/getting_started) provides an easy to understand Python codebase that you can run on any modern computer (including a Raspberry Pi). It captures images from a camera of your choice (by default your webcam) and uses Groundlight to continuously train and deploy a computer vision model that determines if your door is open or closed. Whether you\'re just starting out or a seasoned developer, this example is crafted to provide a smooth introduction to integrating Groundlight into your projects and provide a springboard for building advanced applications with Groundlight. \\n\\nThe `main.py` file could hardly be simpler.  First you just initialize the camera and your Groundlight detector:\\n\\n```python notest\\ncamera = setup_camera()\\n\\ngl = Groundlight()\\n\\nquery_text = \\"Is the door open? This includes if the door is only partially open.\\"\\n\\ndetector_name = \\"door_open_detector\\"\\n\\ndetector = gl.get_or_create_detector(\\n    name=detector_name,\\n    query=query_text,\\n)\\n```\\n\\nand then a simple infinite loop to send images from the camera to your detector:\\n\\n```python notest\\ntry:\\n    while True:\\n        image = camera.grab()\\n\\n        image_query = gl.ask_ml(detector=detector, image=image)\\n        \\n        print(f\\"The answer to the query is {image_query.result.label.value}\\")\\n\\n        sleep(10)\\nfinally:\\n    camera.release()\\n```\\n\\n## How do I get started?\\nVisit the [repository](https://github.com/groundlight/getting_started) and follow the steps in the `README`. After trying it out, we encourage you to modify the code to solve a real world problem you experience. Doing so should be as simple as changing the `query` you ask Groundlight. See the `Learning More - Additional Resources` section of the `README` for more information. If you want to learn more about the Groundlight Python SDK, which is used to power this repository, check out our [SDK](https://github.com/groundlight/python-sdk) or visit the [documentation](https://code.groundlight.ai/python-sdk/docs/getting-started).\\n\\n## We\'re Here to Help!\\nGot questions? We\'re eager to assist! Reach out to us through email (support@groundlight.ai), or chat on the [Groundlight web app](https://app.groundlight.ai) - a Groundlight engineer or scientist is available to help every weekday during business hours.\\n\\nWe can\'t wait to see what you build with Groundlight!"},{"id":"grime-guardian","metadata":{"permalink":"/python-sdk/blog/grime-guardian","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2024-02-01-grime-guardian.md","source":"@site/blog/2024-02-01-grime-guardian.md","title":"The Grime Guardian: Building Stateful Multi-camera applications with Groundlight","description":"Groundlight has a Problem","date":"2024-02-01T00:00:00.000Z","formattedDate":"February 1, 2024","tags":[{"label":"Groundlight Python SDK","permalink":"/python-sdk/blog/tags/groundlight-python-sdk"},{"label":"Raspberry Pi","permalink":"/python-sdk/blog/tags/raspberry-pi"},{"label":"Multithreading","permalink":"/python-sdk/blog/tags/multithreading"},{"label":"State Management","permalink":"/python-sdk/blog/tags/state-management"}],"readingTime":10.895,"hasTruncateMarker":true,"authors":[{"name":"Sunil Kumar","title":"Machine Learning Engineer","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg"}],"frontMatter":{"title":"The Grime Guardian: Building Stateful Multi-camera applications with Groundlight","slug":"grime-guardian","authors":[{"name":"Sunil Kumar","title":"Machine Learning Engineer","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg"}],"tags":["Groundlight Python SDK","Raspberry Pi","Multithreading","State Management"],"image":"./images/grime_guardian/gg_pfp.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Building your first computer vision model just got easier","permalink":"/python-sdk/blog/getting-started"},"nextItem":{"title":"Tales from the Binomial Tail: Confidence intervals for balanced accuracy","permalink":"/python-sdk/blog/confidence-intervals-for-balanced-accuracy"}},"content":"## Groundlight has a Problem  \\nHere at the Groundlight office we have a bit of a problem - sometimes we leave dirty dishes in the office sink. They pile up, and as the pile grows it becomes more and more tempting to simply add to the pile instead of cleaning it up. It was clear that the Groundlight office needed a \u201cgrime guardian\u201d to save us from our messy selves. One day, I realized that this was the perfect problem to solve using Groundlight\u2019s computer vision SDK. I could focus on developing the complex embedded application logic while Groundlight handled the computer vision. My design provided me with an opportunity to test out a handful of interesting design patterns, including deployment on a Raspberry Pi, multi-camera and multi-detector usage, a microservice-like architecture achieved via multithreading, and complex state handling. \\n\\n\\n<figure>\\n    <img src={require(\'./images/grime_guardian/gg_dirty_sink.jpg\').default} />\\n    <figcaption>\\n    <small>\\n    The Groundlight office sink, where dishes accumulate faster than git commits.\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n\x3c!-- truncate --\x3e\\n\\n## Overview of the Application - The Grime Guardian  \\nThe application I developed, the **Grime Guardian**, is designed to make it fun for the Groundlight team to clean up dishes that have been abandoned in the sink ([source code](https://github.com/sunildkumar/GrimeGuardian)). Using two cameras, the application monitors the state of the office sink and the overall kitchen scene. If it recognizes that dirty dishes were left in the sink for over a minute, it posts a funny yet inspiring message and photo to a Discord server that alerts the Groundlight team and encourages someone to help. Then, while the dishes remain unattended it surveys the kitchen until it sees someone. Once someone comes to help, it posts a message and photo, celebrating them as a hero, giving everyone in the Discord server a chance to recognize them. While this is cheesy, it has made it a bit more fun for us to do the dishes!\\n\\n<figure>\\n    <img src={require(\'./images/grime_guardian/gg_dirty_sink_notification.png\').default} />\\n    <figcaption>\\n    <small>\\n     The Grime Guardian alerting the Groundlight Team through Discord \\n    </small>\\n    </figcaption>\\n</figure>\\n\\n## Architecture of a Sophisticated Groundlight Application\\nThe Grime Guardian demonstrates how to build an advanced Groundlight application in a handful of ways:\\n1. **Raspberry Pi Deployment** - The Grime Guardian leverages our custom [Raspberry Pi Image](https://github.com/groundlight/groundlight-pi-gen), which makes it easy to deploy Groundlight applications on Raspberry Pi. \\n2. **Multiple Cameras** - The Grime Guardian actively uses more than one camera to solve a problem (it has one camera pointed at the sink and one pointed at the general kitchen scene). \\n3. **Multiple Detectors** - The Grime Guardian combines multiple Groundlight detectors to solve a problem.\\n4. **Microservice-like architecture via Multithreading** - The Grime Guardian\u2019s architecture is broken down into a handful of microservice-like processes - each running in a different thread on the same machine. This improves the app\u2019s robustness and allows for more flexibility and scalability. \\n5. **Complex State** - As described in the previous section, the state of the world this app is tracking is somewhat complex. In addition to knowing the state of the sink and kitchen, the app tracks how recently the state was updated and how recently it has sent a notification to the Groundlight team. \\n6. **Discord Bot Integration/Notifications** - The Grime Guardian uses the Discord Bot API to send notifications to a Discord server. Discord can be an extremely powerful and flexible tool for building applications (e.g. Midjourney). \\n7. **Robustness** - In practice, the Grime Guardian has been extremely robust, with only one or two incorrect (false positive) notifications over many weeks of deployment and hundreds of thousands of Groundlight queries. \\n\\n## Microservice-like Architecture\\nThe Grime Guardian leverages a microservice-like architecture via multithreading to enhance its performance and robustness. Each microservice within the application runs in its own thread on a single Raspberry Pi, allowing for simultaneous execution of tasks. This architecture is particularly beneficial in this context as it allows the application to monitor the sink and the kitchen scene concurrently using two cameras, and to process the data from these cameras independently. Furthermore, it enables the application to manage complex state tracking and Discord notifications without blocking or slowing down the image processing tasks. \\n\\nThe application is broken into six microservices:\\n1. **Sink Image Capturer**: This microservice captures images from a camera pointed at the sink and submits them as queries to a Groundlight detector via the `ask_async` SDK method (this method is useful for times in which the thread submitting image queries is not the same thread that will be retrieving and using the results). I set the detector\'s query to \\"Is there at least one dish in the sink? Cleaning supplies like a sponge, brush, soap, etc. are not considered dishes. If you cannot see into the sink, consider it empty and answer NO\\" and set the confidence threshold to 75%. After Groundlight replies with a query ID, the service passes the query ID to the **Query Processor** service.  \\n2. **Kitchen Image Capturer**: This microservice is identical to the **Sink Image Capturer** except it uses the camera that can view the whole kitchen and submits images to a detector with the query \\"Is there at least one person in this image?\\" and set the confidence threshold to 75% as well. \\n3. **Query Processor**: This microservice processes the queries passed to it by the two **Capturer** services, waiting for confident answers from Groundlight and filtering out queries that do not become confident within a reasonable time (I chose a 10 second timeout as that was how frequently each **Capturer** service submitted a query to Groundlight). Queries that become confident are passed to the **State Updater** service.\\n4. **State Updater**: This microservice updates a complex model of the application\'s state based on Groundlight\'s responses. It tracks the status and last update time of the sink and kitchen, the image query IDs that led to the current state, and the timestamps of the last clean sink and notifications sent. \\n5. **Notification Publisher**: This microservice listens for updates to the state of the application (written by the **State Updater**) and decides whether it is appropriate to send one of two possible notifications. If a notification is needed, it adds it to a queue of notifications to be processed by the **Discord Bot**. Importantly, the **Notification Publisher** only determines if a notification should be sent. It does not handle the mechanics of what data to send or how and where to send it. \\n6. **Discord Bot**: This microservice runs a Discord bot, which listens for requests from the **Notification Publisher**. When a request arrives, the bot collects the relevant data and sends notifications to a Discord server.\\n\\n\\n<figure>\\n    <img src={require(\'./images/grime_guardian/gg_arch_flow.png\').default} />\\n    <figcaption>\\n    <small>\\n     <small>Diagram created by Jared Randall</small><br/>Architecture diagram for the application\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n## State Management and Notification Logic\\nThe Grime Guardian\'s ability to track and manage a complex state is a cornerstone of its functionality. The application not only needs to know the current state of the sink and kitchen but also when these states were last updated and when the last notifications were sent. In total, the application needs nine separate variables to function properly (a combination of binary-encoded state fields, timestamps, and image query IDs).  This level of detail is crucial for avoiding redundant alerts and ensuring timely and accurate updates. \\n\\nAs seen in the architecture diagram in the previous section, multiple services read and write to the state simultaneously. To handle this complexity, I implemented a wrapper around the state to handle reads and writes in a thread safe manner. This wrapper ensures the state can be accessed and modified safely across many services. It uses a lock to prevent race conditions, ensuring that only one thread can modify the state at a time.\\n\\n```python\\nimport threading\\nimport copy\\n\\n# simplified version of how the Grime Guardian manages state safely\\nclass SimpleThreadSafeState:\\n    def __init__(self):\\n        self.state = False\\n        self.lock = threading.Lock()\\n\\n    def update_state(self, new_state: bool):\\n        with self.lock:\\n            self.state = new_state\\n\\n    def get_state(self) -> bool:\\n        with self.lock:\\n            return copy.copy(self.state)\\n```\\n\\nThe application uses this state to determine when to send notifications. I\'ve tried to break down this logic into a few of flowcharts. At a high level, the logic is pretty simple. Whenever the the application\'s state is updated, the application performs a check to determine if the new state justifies sending each type of notification.\\n\\n<figure>\\n    <img src={require(\'./images/grime_guardian/gg_notification_flow.png\').default} />\\n    <figcaption>\\n    <small>\\n     <small>Diagram created by Jared Randall</small><br/>High level flow for determining if a notification should be sent\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nThe logic for determining if each notification should be sent is a bit more complex. It first checks for the last time a notification was sent. If the last notification was sent in the last 5 minutes, no notification is sent. This is important as it prevents the application from spamming the Discord server with notifications. Next, the application checks if the sink currently has dirty dishes in it, and how long it has been since the sink was empty. We only send the notification if dirty dishes have been present for more than a minute. This approach ensures that the Grime Guardian does not send a notification every time someone puts a dirty dish in the sink, but only when dishes have been abandoned for a while. This ensures that the app only notifies the team when it is actually needed.\\n\\n<figure>\\n    <img src={require(\'./images/grime_guardian/gg_dishes_notification_flow.png\').default} />\\n    <figcaption>\\n    <small>\\n     <small>Diagram created by Jared Randall</small><br/> Flow for determining if the dirty dishes notification should be sent\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nThe logic for determining if someone has arrived to help is similar. We have a check that ensures we do not spam the Discord server. Then, we only send a notification if there are currently dishes in the sink and someone is present in the kitchen. This ensures that the Grime Guardian does not send a notification every time someone walks into the kitchen, but only when dishes are in the sink. \\n\\n<figure>\\n    <img src={require(\'./images/grime_guardian/gg_help_arrived_flow.png\').default} />\\n    <figcaption>\\n    <small>\\n     <small>Diagram created by Jared Randall</small><br/> Flow for determining if the help arrived notification should be sent\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nIn retrospect, getting the notification logic to work properly was one of the more challenging parts of this project. The version I presented here is the result of many iterations and tweaks based on real world usage and results. I think this is because this logic is an expression of the application\'s core value proposition. If this \\"business logic\\" is not correct, the application will not be fun or useful. Fortunately, Groundlight enabled me to focus on this logic and not worry about the computer vision. \\n\\n## Discord Bot Notifications\\nThe Grime Guardian uses the [Discord Bot API](https://discord.com/developers/docs/intro) to send notifications to a Discord server I set up. At startup, Discord requires some boilerplate to handle authentication. After this is done, the bot listens for new notification requests from the **Notification Publisher**. Based on the type of request, the bot collects the relevant information (e.g. the image of the dirty sink, or the person doing the dishes) and sends the message. The Discord Bot API makes this incredibly simple, after handling authentication, a new message and an attached image can be sent in a single line.\\n\\n```python\\nawait channel.send(\\"message\\", file=discord.File(fpath))\\n```\\nWhile I did not have time to add more complexity to the bot, Discord\u2019s strong documentation gives me confidence it would not be that hard to add more features. For example, it would have been nice if the bot could listen for replies or emote reactions to its notifications - if the bot reported that the sink was full of dishes when really it was not, I could react to the notification with an emote that indicates the correct label for the image, and then the bot could automatically send this information to Groundlight, improving ML performance. \\n\\n## Future Improvements and Enhancements\\nExtending the functionality of the application, I can imagine adding motion detection to limit the frequency of image submissions to Groundlight. Currently, the application sends images to Groundlight at a fixed interval (every 10 seconds), regardless of whether there has been any significant change in the scene. This approach, while simple, could be optimized to become more cost effective. As it is now, it can lead to unnecessary image submissions when the scene is static. By incorporating motion detection, the application could intelligently decide when to send images to Groundlight. Fortunately, some of my excellent colleagues have built [`framegrab`](https://github.com/groundlight/framegrab/), an open source tool that automatically handles this.\\n\\n## Build Your Own Grime Guardian\\nThank you for taking the time to read my post! As I reflect back, I\u2019m very proud of how Groundlight enabled me to very quickly and effortlessly stand up an ML solution to solve a simple office problem in a fun and engaging way! If you are particularly interested or inspired, I encourage you to check out the [source code](https://github.com/sunildkumar/GrimeGuardian). Feel free to open a GitHub issue with questions or submit a PR with improvements! \\n\\n<figure>\\n    <img src={require(\'./images/grime_guardian/gg_hero_notification.png\').default} />\\n    <figcaption>\\n    <small>\\n       The Grime Guardian celebrates Tom, my colleague, for his heroic cleaning effort. The grime is no match for his dish-defeating determination!\\n    </small>\\n    </figcaption>\\n</figure>"},{"id":"confidence-intervals-for-balanced-accuracy","metadata":{"permalink":"/python-sdk/blog/confidence-intervals-for-balanced-accuracy","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2024-01-16-binomial-tails.md","source":"@site/blog/2024-01-16-binomial-tails.md","title":"Tales from the Binomial Tail: Confidence intervals for balanced accuracy","description":"How we assess and report detector accuracy","date":"2024-01-16T00:00:00.000Z","formattedDate":"January 16, 2024","tags":[{"label":"metrics","permalink":"/python-sdk/blog/tags/metrics"},{"label":"math","permalink":"/python-sdk/blog/tags/math"},{"label":"deep dive","permalink":"/python-sdk/blog/tags/deep-dive"}],"readingTime":23.39,"hasTruncateMarker":true,"authors":[{"name":"Ted Sandler","title":"Senior Applied Scientist at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/efc35da152/sandlert.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/efc35da152/sandlert.jpg"},{"name":"Leo Dirac","title":"CTO and Co-founder at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png","imageURL":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png"}],"frontMatter":{"title":"Tales from the Binomial Tail: Confidence intervals for balanced accuracy","description":"How we assess and report detector accuracy","slug":"confidence-intervals-for-balanced-accuracy","authors":[{"name":"Ted Sandler","title":"Senior Applied Scientist at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/efc35da152/sandlert.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/efc35da152/sandlert.jpg"},{"name":"Leo Dirac","title":"CTO and Co-founder at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png","imageURL":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png"}],"tags":["metrics","math","deep dive"],"image":"./images/binomial-tails/binomial_confidence_intervals__muted_theme__social.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"The Grime Guardian: Building Stateful Multi-camera applications with Groundlight","permalink":"/python-sdk/blog/grime-guardian"},"nextItem":{"title":"Linux OS Images for Computer Vision on Raspberry Pi","permalink":"/python-sdk/blog/raspberry-pi-computer-vision"}},"content":"At Groundlight, we put careful thought into measuring the correctness of our machine learning detectors. In the simplest case, this means measuring detector accuracy. But our customers have vastly different performance needs since our platform allows them to train an ML model for nearly any Yes/No visual question-answering task. A single metric like accuracy is unlikely to provide adequate resolution for all such problems. Some customers might care more about false positive mistakes (precision) whereas others might care more about false negatives (recall).\\n\\n\x3c!-- truncate --\x3e\\n\\nTo provide insight for an endless variety of use cases yet still summarize performance with a single number, Groundlight\'s **accuracy details** view displays each detector\'s [balanced accuracy](https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score). Balanced accuracy is the average of recall for all classes and is Groundlight\'s preferred summary metric. For binary problems, this is just the mean of accuracy on the should-be-YES images and accuracy on the should-be-NOs. We prefer balanced accuracy because it is easier to understand than metrics like the F1 score or AUROC. And since many commercially interesting problems are highly imbalanced - that is the answer is almost always YES or always NO - standard accuracy is not a useful performance measure because always predicting the most common class will yield high accuracy but be useless in practice.\\n\\n<figure>\\n    <img src={require(\'./images/binomial-tails/streecar-visible-accuracy-details.png\').default} />\\n    <figcaption>\\n    <small>\\n      <b>Figure 1:</b> the detector accuracy details view shows balanced accuracy and per-class accuracy with exact 95% confidence intervals\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nHowever, we\'ve found that just displaying the balanced accuracy is not informative enough, as we do not always have an ample supply of **ground truth** labeled images to estimate it from. Ground truth labels are answers to image queries that have been provided by a customer, or customer representative, and are therefore trusted to be correct. With only a few ground truth labels, the estimate of a detector\'s balanced accuracy may itself be inaccurate. As such, we find it helpful to quantify and display the degree of possible inaccuracy by constructing confidence intervals for balanced accuracy, which brings us to the subject of this blog post!\\n\\nAt Groundlight, we compute and display **exact** confidence intervals in order to upper and lower bound each detector\'s balanced accuracy, and thereby convey the amount of precision in the reported metric. The detector\'s accuracy details view displays these intervals as colored bars surrounding the reported accuracy numbers (see figure 1, above). This blog post describes the mathematics behind how we compute the intervals using the tails of the binomial distribution, and it also strives to provide a healthy amount of intuition for the math.\\n\\nUnlike the approximate confidence intervals based on the Gaussian distribution, which you may be familiar with, confidence intervals based on the binomial tails are exact, regardless of the number of ground truth labels we have available. Our exposition largely follows [Langford, 2005](https://jmlr.org/papers/v6/langford05a.html) and we use his \\"program bound\\" as a primitive to construct confidence intervals for the balanced accuracy metric.\\n\\n## Background\\n\\nTo estimate and construct confidence intervals for balanced accuracy, we first need to understand how to construct confidence intervals for standard \\"plain old\\" accuracy. So we\'ll start here.\\n\\nRecall that standard accuracy is just the fraction of predictions a classifier makes which happen to be correct. This sounds simple enough, but to define this fraction rigorously, we actually need to make assumptions. To see why, consider the case that our classifier performs well on daytime images but poorly on nighttime ones. If the stream of images consists mainly of daytime photos, then our classifier\'s accuracy will be high, but if it\'s mainly nighttime images, our classifier\'s accuracy will be low. Or if the stream of images drifts slowly over time from day to nighttime images, our classifier won\'t even have a single accuracy. Its accuracy will be time-period dependent.\\n\\nTherefore, a classifier\'s \\"true accuracy\\" is inherently a function of the distribution of examples it\'s applied to. In practice, we almost never know what this distribution is. In fact, it\'s something of a mathematical fiction. But it happens to be a useful fiction in so far as it reflects reality, in that it lets us do things like bound the Platonic true accuracy of a classifier and otherwise reason about out-of-sample performance. Consequently, we make the assumption that there exists a distribution over the set of examples that our classifier sees, and that this distribution remains fixed over time.\\n\\nLet\'s call the distribution over images that our classifier sees, $D$. Each example in $D$ consists of an image, $x \\\\in \\\\mathcal{X}$, and an associated binary label, $y \\\\in$ \\\\{ YES, NO \\\\}, which is the answer to the query. Let $(x,y) \\\\sim D$ denote the action of sampling an example from $D$. We conceptualize our machine learning classifier as a function, $h$, which maps from the set of images, $\\\\mathcal{X}$, to the set of labels, $\\\\mathcal{Y}$. We say that $h$ correctly classifies an example $(x,y)$ if $h(x) = y$, and that $h$ misclassifies it otherwise.\\n\\nFor now, our goal is to construct a confidence inverval for the true, but unknown, accuracy of $h$. We define this true accuracy as the probability that $h$ correctly classifies an example drawn from $D$:\\n$$\\n \\\\text{acc}_{D}(h) = \\\\Pr_{(x,y) \\\\sim D}[ \\\\,h(x) = y\\\\, ].\\n$$\\n\\nThe true accuracy is impossible to compute exactly because $D$ is unknown and the universe of images is impossibly large. However, we can estimate it by evaluating $h$ on a finite set of test examples, $S$, which have been [drawn i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) from $D$. That is,\\n$$\\nS = \\\\{ (x_1, y_1), (x_2, y_2), ..., (x_{n}, y_{n}) \\\\}\\n$$\\nwhere each $(x_i, y_i) \\\\sim D$ for $i=1,\\\\ldots,n$.\\n\\nThe fraction of images in $S$ that $h$ correctly classifies is called $h$\'s empirical accuracy on $S$, and this fraction is computed as\\n$$\\n\\\\widehat{\\\\text{acc}}_{S}(h) = \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\mathbf{1}[\\\\, h(x_i) = y_i \\\\,].\\n$$\\n\\nThe notation $\\\\mathbf{1}[\\\\, \\\\texttt{condition} \\\\,]$ is shorthand for the indicator function which equals 1 when the $\\\\texttt{condition}$ is true and 0 otherwise. So the formula above just sums the number of examples in $S$ that are correctly classified and then multiplies by 1/n.\\n\\nThe egg-shaped infographic below depicts the scenario of estimating $h$\'s true accuracy from its performance on a finite test set. The gray ellipse represents the full distribution of examples, $D$. Each dot corresponds to a single example image, $x$, whose true label, $y$, is represented by the dot\'s color - red for YES and blue for NO. The classifier, $h$, is represented by the dotted black line. Here, $h$ is the decision rule that classifies all points to the left of the line as should-be YES and all points to the right as should-be-NO. The points with light gray circles around them are the ones that have been sampled to form the test set, $S$.\\n\\n<figure>\\n    <img src={require(\'./images/binomial-tails/generalization.png\').default} width=\\"360px\\" />\\n    <figcaption>\\n    <small>\\n      <b>Figure 2:</b> true accuracy can only be estimated from performance on a finite test set. The gray shaded region\\n      represents the full distribution. The lightly circled points are examples sampled for the test set.\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nIn this case, our choice of test set, $S$, was unlucky because $h$\'s empirical accuracy on $S$ looks great, appearing to be 9/9 = 100%. But evaluating $h$ on the full distribution of examples, $D$, reveals that its true accuracy is much lower, only 24/27 = 89%. If our goal is to rarely be fooled into thinking that $h$\'s performance is much better than it really is, then this particular test set was unfortunate in the sense that $h$ performs misleadingly well.\\n\\n\\n## Test Set Accuracy and Coin Flips\\n\\nIt turns out that the problem of determining a classifier\'s true accuracy from its performance on a finite test set exactly mirrors the problem of determining the bias of a possibly unfair coin after observing some number of flips. In this analogy, the act of classifying an example corresponds to flipping the coin, and the coin landing heads corresponds to the classifier\'s prediction being correct.\\n\\nUsefully, the binomial distribution completely characterizes the probability of observing $k$ heads in $N$ independent tosses of a biased coin whose bias, or propensity to land heads, is known to be the probability, $p$, through its probability mass function (PMF), defined as\\n$$\\nf_{N,p}(k) = {N \\\\choose k} p^k (1 - p)^{N-k}.\\n$$\\n\\nThe cumulative density function (CDF) is the associated function that sums up the PMF probabilities over all outcomes (i.e., number of heads) from 0 through $k$. It tells us the probability of observing $k$ or fewer heads in $N$ independent tosses when the coin\'s bias is the probability $p$. The CDF is defined as\\n$$\\nF_{N,p}(k) = \\\\sum_{j = 0}^k f_{N,p}(k).\\n$$\\n\\nBelow we\'ve plotted the PMF (left) and CDF (right) functions for a binomial distribution whose parameters are *N*=30 and *p*=0.3.\\n<table>\\n<tr>\\n  <td><img src={require(\'./images/binomial-tails/binomial_pmf.png\').default} width=\\"350px\\" /></td>\\n  <td><img src={require(\'./images/binomial-tails/binomial_cdf.png\').default} width=\\"350px\\" /></td>\\n</tr>\\n</table>\\n\\nThe PMF looks like a symmetric \\"bell curve\\". Its x-axis is the number of tosses that are heads, $k$. And its y-axis is the probability of observing $k$ heads in $N$ tosses. The CDF plot shows the cumulative sum of the PMF probabilities up through $k$ on its y-axis. The CDF is a monotonically increasing function of $k$. Its value is 1.0 on the right side of the plot since the sum of all PMF probabilities must equal one.\\n\\nThe binomial PMF doesn\'t always resemble a bell-shaped curve. This is true of the binomial distributions in the two plots below, whose respective bias parameters are p=0.15 and p=0.96.\\n<table>\\n<tr>\\n  <td><img src={require(\'./images/binomial-tails/binomial_pmf_N20_p015.png\').default} width=\\"400px\\" /></td>\\n  <td><img src={require(\'./images/binomial-tails/binomial_pmf_N20_p096.png\').default} width=\\"400px\\" /></td>\\n</tr>\\n</table>\\n\\n## Upper Bounding the True Accuracy from Test Set Performance\\n\\nNow that we\'ve examined the probability of coin tossing and seen how the number of heads from tosses of a biased coin mirrors the number of correctly classified examples in a randomly sampled test set, let\'s consider the problem of determining an upper bound for the true accuracy of a classifier given its performance on a test set.\\n\\nImagine that we\'ve sampled a test set, $S$, from $D$ with 100 examples, and that our classifier, $h$, correctly classified 80 of them. We would like to upper bound $h$\'s true accuracy, $\\\\text{acc}_D(h)$, having observed its empirical accuracy, $\\\\widehat{\\\\text{acc}}_S(h)$ = 80/100 = 80%.\\n\\nLet\'s start by considering a very naive choice for the upper bound, taking it to equal the empirical accuracy of 80%.\\n\\nThe figure below plots the PMF of a binomial distribution with parameters *N*=100 and *p*=0.80. Here, *N* is the test set size and *p* corresponds to the true, but unknown, classifier accuracy. The plot shows that if our classifier\'s true accuracy were in fact 80%, there would be a very good chance of observing an even lower empirical accuracy than what we actually observed. This is reflected in the substantial amount of probability mass lying to the left of the purple vertical line, which is placed at the empirical accuracy point of 80/100 = 80%.\\n\\n<figure>\\n    <img src={require(\'./images/binomial-tails/binomial_pmf_true80_emp80.png\').default} />\\n    <img src={require(\'./images/binomial-tails/binomial_cdf_true80_emp80.png\').default} />\\n    <figcaption>\\n    <small>\\n      <b>Figure 3:</b> Binomial PMF (top) and CDF (bottom) for N=100 and true accuracy 80.0%. The CDF shows there is a 54% chance of seeing an empirical accuracy of 80% or less.\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nIn fact, the CDF of the binomial tells us that there is a 54% chance of seeing an empirical accuracy of 80% ***or less*** when the true accuracy is 80%. And since 54% is fairly good odds, our naive choice of 80% as an upper bound doesn\'t appear very safe. It would therefore be wise to increase our upper bound if we want it to be an upper bound!\\n\\nIn contrast, the plot below shows that if the true accuracy were a bit higher, say 83%, we would only have a 1 in 4 chance of observing an empirical accuracy less than or equal to our observed accuracy of 80%. Or put differently, roughly a quarter of the test sets we could sample from $D$ would yield an empirical accuracy of 80% or lower if $h$\'s true accuracy was 83%. This is shown by the 24.8% probability mass located to the left of the purple line at the 80% empirical accuracy point. The red line is positioned at the hypothesized true accuracy of 83%.\\n<figure>\\n    <img src={require(\'./images/binomial-tails/binomial_pmf_true83_emp80.png\').default} />\\n    <img src={require(\'./images/binomial-tails/binomial_cdf_true83_emp80.png\').default} />\\n    <figcaption>\\n    <small>\\n      <b>Figure 4:</b> Binomial PMF (top) and CDF (bottom) for N=100 and true accuracy 83.0%. The CDF shows there is a 24.8% chance of seeing an empirical accuracy of 80% or less.\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nStill, events with one in four odds are quite common, so hypothesizing an even larger true accuracy would be wise if we want to ensure it\'s not less than the actual true accuracy.\\n\\nThe next plot shows that if the true accuracy were higher still, at 86.3%, the empirical accuracy of 80% or less would be observed on only 5% of sampled test sets. This is evidenced by the even smaller amount of probability mass to the left of the purple line located at the empirical accuracy of 80%. Again, the red line is positioned at the hypothesized true accuracy of 86.3%.\\n\\n<figure>\\n    <img src={require(\'./images/binomial-tails/binomial_pmf_true86_emp80.png\').default} />\\n    <img src={require(\'./images/binomial-tails/binomial_cdf_true86_emp80.png\').default} />\\n    <figcaption>\\n    <small>\\n      <b>Figure 5:</b> Binomial PMF (top) and CDF (bottom) for N=100 and true accuracy 86.3%. The CDF shows there is a 5% chance of seeing an empirical accuracy of 80% or less.\\n    </small>\\n    </figcaption>\\n</figure>\\n\\nIn other words, if $h$\'s true accuracy were 86.3% or greater, we\'d observe an empirical accuracy of 80% or lower on just 1 in 20 test sets. Consequently, the hypothesized true accuracy of 86.3% seems like a pretty safe choice for an upper bound.\\n\\n\\n### Constructing a 95% Upper Confidence Bound\\n\\nThe procedure we just outlined, of increasing the hypothesized true accuracy starting from the observed empirical accuracy until exactly 5% of the binomial\'s probability mass lies to the left of the empirical accuracy, is how we construct an exact 95% upper confidence bound for the true accuracy.\\n\\nRemarkably, if we apply this procedure many times to find 95% accuracy upper confidence bounds for different ML classifiers at Groundlight, the computed upper bounds will in fact be larger than the respective classifiers\' true accuracies in 95% of these encountered cases. This last statement is worth mulling over because it is exactly the right way to think about the guarantees associated with upper confidence bounds.\\n\\nRestated, a 95% upper confidence bound procedure for the true accuracy is one that produces a quantity greater than the true accuracy 95% of the time.\\n\\n## Exact Upper Confidence Bounds based on the Binomial CDF\\n\\nSo now that we\'ve intuitively described the procedure used to derive exact upper confidence bounds, we give a more formal treatment that will be useful in discussing confidence intervals for balanced accuracy.\\n\\nFirst, recall that the binomial\'s CDF function, $F_{N,p}(k)$, gives the probability of observing $k$ or fewer heads in $N$ tosses of a biased coin whose bias is $p$.\\n\\nAlso, recall in the previous section that we decided to put exactly 5% of the probability mass in the lower tail of the PMF, and this yielded a 95% upper confidence bound. But we could have placed 1% in the lower tail, and doing so would have yielded a 99% upper confidence bound. A 99% upper confidence bound is looser than a 95% upper bound, but it upper bounds the true accuracy on 99% of test sets sampled as opposed to just 95%.\\n\\nThe tightness of the bound versus the fraction of test sets it holds for is a trade off that we get to make referred to as the *coverage*. We control the coverage through a parameter named $\\\\delta$. Above we had set $\\\\delta$ to 5% which gave us a 1 - $\\\\delta$ = 95% upper confidence bound. But we could have picked some other value for $\\\\delta$.\\n\\nWith $\\\\delta$ understood, we are now ready to give our formal definition of upper confidence bounds. Let $\\\\delta$ be given, $N$ be the number of examples in the test set, $k$ be the number of correctly classified test examples, and $p$ be the true accuracy.\\n\\n**Definition:** the 100(1 - $\\\\delta$)% binomial upper confidence bound for $p$ is defined as\\n$$\\n\\\\bar{p}(N, k, \\\\delta) = \\\\max \\\\{ \\\\, p \\\\,:\\\\, F_{N,p}(k) \\\\ge \\\\delta \\\\,\\\\, \\\\}.\\n$$\\nIn words, $\\\\bar{p}$ is the maximum accuracy for which there exists at least $\\\\delta$ probability mass in the lower tail lying to the left of the observed number of correct classifications for the test set. And this definition exactly mirrors the procedure we used above to find the 95% upper confidence bound. We picked $\\\\bar{p}$ to be the max $p$ such that the CDF $F_{N=100,p}(k=80)$ was at least $\\\\delta$ = 5%. \\n\\nWe can easily implement this definition in code. The binomial CDF is available in python through the **[scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html)** module as **[binom.cdf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html#scipy.stats.binom)**. And we can use it to find the largest value of $p$ for which $F_{N,p}(k) \\\\ge \\\\delta$. However the CDF isn\'t directly invertible, so we can\'t just plug in $\\\\delta$ and get $\\\\bar{p}$ out. Instead we need to search over possible values of $p$ until we find the largest one that satisfies the inequality.  This can be done efficiently using the interval bisection method which we implement below.\\n\\n```python notest\\nfrom scipy.stats import binom\\n\\ndef binomial_upper_bound(N, k, delta):\\n    \\"\\"\\"\\n    Returns a 100*(1 - delta)% upper confidence bound on the accuracy\\n    of a classifier that correctly classifies k out of N examples.\\n    \\"\\"\\"\\n    def cdf(p):\\n        return binom.cdf(k, N, p)\\n\\n    def search(low, high):\\n        if high - low < 1e-6:\\n            return low  # we have converged close enough\\n        mid = (low + high) / 2\\n        if cdf(mid) >= delta:\\n            return search(mid, high)\\n        else:\\n            return search(low, mid)\\n\\n    return search(low=k/N, high=1.0)\\n```\\n\\n## Lower Confidence Bounds\\n\\nReferring back to our discussion of coin flips makes it clear how to construct lower bounds for true accuracy. We had likened a correct classification to a biased coin landing heads and we upper bounded the probability of heads based on the observed number of heads.\\n\\nBut we could have used the same math to upper bound the probability of tails. And likening tails to misclassifications lets us upper bound the true error rate. Moreover, the error rate equals one minus the accuracy. And so we immediately get a lower bound on the accuracy by computing an upper bound on the error rate and subtracting it from one.\\n\\nAgain, let $\\\\delta$ be given, $N$ be the number of test examples, $k$ be the number of correctly classified test examples, and let $p$ be the true, but unknown, accuracy.\\n\\n**Definition:** the 100(1 - $\\\\delta$)% binomial lower confidence bound for $p$ is defined as\\n$$\\n\\\\underline{p}(N, k, \\\\delta) = 1 - \\\\max \\\\{ \\\\, p \\\\,:\\\\, F_{N,p}(N - k) \\\\ge \\\\delta \\\\,\\\\, \\\\}.\\n$$\\nHere $N - k$ is the number of misclassified examples observed in the test set.\\n\\n\\n## Central Confidence Intervals\\n\\nNow that we know how to derive upper and lower bounds which hold individually at a given confidence level, we can use our understanding to derive upper and lower bounds which hold simultaneously at the given confidence level. To do so, we compute what is called a *central confidence interval*. A 100$\\\\times$(1 - $\\\\delta$)% central confidence interval is computed by running the upper and lower bound procedures with the adjusted confidence level of 100$\\\\times$(1 - $\\\\delta$/2)%.\\n\\nFor example, if we want to compute a 95% central confidence interval, we compute 97.5% lower and upper confidence bounds. This places $\\\\delta$/2 = 2.5% probability mass in each tail, thereby providing 95% coverage in the central region.\\n\\nPictorially below, you can see that the 95% central confidence interval (top row) produces wider bounds than just using the 95% lower and upper confidence bounds separately (bottom row). The looser bounds are unfortunate. But naively computing the lower and upper bounds at the original confidence level of 95% sacrifices coverage due to [multiple testing](https://en.wikipedia.org/wiki/Multiple_comparisons_problem).\\n\\n<figure>\\n  <img src={require(\'./images/binomial-tails/binomial_confidence_intervals__muted_theme.png\').default} style={{ height:\\"400px\\" }} />\\n  <figcaption>\\n  <small>\\n    <b>Figure 6:</b> central confidence intervals produce wider bounds to correct for multiple testing\\n  </small>\\n  </figcaption>\\n</figure>\\n\\nIn the next section, where we compute central confidence intervals for balanced accuracy, we will have to do even more to correct for multiple testing.\\n\\n## Confidence Bounds for Balanced Accuracy\\n\\nRecall that the balanced accuracy for a binary classifier is the mean of its accuracy on examples from the positive class and its accuracy on examples from the negative class.\\n\\nTo define what we mean by the \\"true balanced accuracy\\", we need to define appropriate distributions over examples from each class. To do so, we decompose $D$ into separate class conditional distributions, $D^+$ and $D^-$, where\\n$$\\n\\\\Pr\\\\left\\\\{ (x,y) \\\\sim D^+ \\\\right\\\\} = \\\\Pr\\\\left\\\\{ (x,y) \\\\sim D \\\\mid y = +1 \\\\right\\\\},\\n$$\\n$$\\n\\\\Pr\\\\left\\\\{ (x,y) \\\\sim D^- \\\\right\\\\} = \\\\Pr\\\\left\\\\{ (x,y) \\\\sim D \\\\mid y = -1 \\\\right\\\\}.\\n$$\\n\\nThe positive and negative true accuracies are defined with respect to each of these class specific distributions:\\n$$\\n\\\\text{acc}^+(h) = E_{(x,y) \\\\sim D^+} \\\\, \\\\mathbf{1}[ h(x_i) = y_i ],\\n$$\\n$$\\n\\\\text{acc}^-(h) = E_{(x,y) \\\\sim D^-} \\\\, \\\\mathbf{1}[ h(x_i) = y_i ].\\n$$\\n\\nThe true balanced accuracy is then defined as the average of these,\\n$$\\n\\\\text{acc}_\\\\text{bal}(h) = \\\\frac{\\\\text{acc}^+(h) + \\\\text{acc}^-(h)}{2}.\\n$$\\n\\n\\n### Constructing the Bound for Balanced Accuracy\\n\\nWith the above definitions in hand, we can now bound the balanced accuracy of our classifier based on its performance on a test set. Let $S$ be the test set, and let\\n* $N^+$ denote the number of positive examples in $S$\\n* $N^-$ denote the number of negative examples in $S$\\n* $k^+$ denote the number of positive examples in $S$ that $h$ correctly classified\\n* $k^-$ denote the number of negative examples in $S$ that $h$ correctly classified\\n\\nFrom these quantities, we can find lower and upper bounds for the positive and negative accuracies based on the binomial CDF.\\n\\nDenote these lower and upper bounds on positive and negative accuracy as\\n$$\\n    \\\\underline{\\\\text{acc}^+}(h)\\n,~~ \\\\overline{\\\\text{acc}^+}(h)\\n,~~ \\\\underline{\\\\text{acc}^-}(h)\\n,~~ \\\\overline{\\\\text{acc}^-}(h).\\n$$\\n\\nTo find a 100(1 - $\\\\delta$)% confidence interval for the $\\\\text{acc}_\\\\text{bal}(h)$, we first compute the quantities\\n$$\\n\\\\underline{\\\\text{acc}^+}(h) = \\\\underline{p}(N^+, k^+, \\\\delta/4)\\n~~ \\\\text{ and } ~~\\n\\\\overline{\\\\text{acc}^+}(h) = \\\\overline{p}(N^+, k^+, \\\\delta/4)\\n$$\\n$$\\n\\\\underline{\\\\text{acc}^-}(h) = \\\\underline{p}(N^-, k^-, \\\\delta/4)\\n~~ \\\\text{ and } ~~\\n\\\\overline{\\\\text{acc}^-}(h) = \\\\overline{p}(N^-, k^-, \\\\delta/4)\\n$$\\n\\nImportantly, we\'ve used an adjusted delta value of $\\\\delta/4$ to account for mulitple testing. That is, if we desire our overall coverage to be (1 - $\\\\delta$) = 95%, we run our individual bounding procedures with the substituted delta value of $\\\\delta/4 = 1.25\\\\%$.\\n\\nThe reason why is as follows. By construction, each of the four bounds will fail to hold with probability $\\\\delta/4$. The union bound in **appendix A** tells us that the probability of at least one of these four bounds failing is no greater than the sum of the probabilities that each fails. Summing up the failure probabilities for all four bounds, the probability that at least one bound fails is therefore no greater than $4\\\\cdot(\\\\delta/4) = \\\\delta$. Thus the probability that none of the bounds fails is at least 1 - $\\\\delta$, giving us the desired level of coverage.\\n\\nLast, we obtain our exact lower and upper bounds for balanced accuracy by averaging the respective lower and upper bounds for the positive and negative class accuracies:\\n$$\\n\\\\underline{\\\\text{acc}_\\\\text{bal}}(h) = (1/2) \\\\left(\\n\\\\underline{\\\\text{acc}^+}(h) + \\\\underline{\\\\text{acc}^-}(h)\\n\\\\right)\\n$$\\n$$\\n\\\\overline{\\\\text{acc}_\\\\text{bal}}(h) = (1/2) \\\\left(\\n\\\\overline{\\\\text{acc}^+}(h) + \\\\overline{\\\\text{acc}^-}(h)\\n\\\\right)\\n$$\\n\\nPictorially below, we can see how the averaged lower and upper bounds contain the true balanced accuracy.\\n\\n<figure>\\n    <img src={require(\'./images/binomial-tails/balanced-accuracy-bound.png\').default} width=\\"500px\\" />\\n    <figcaption>\\n    <small>\\n      <b>Figure 7:</b> the balanced accuracy is bounded by the respective averages of the lower and upper bounds\\n    </small>\\n    </figcaption>\\n</figure>\\n\\n\\n## Comparison with intervals based on the Normal approximation\\n\\nThe main benefit of using bounds derived from the binomial CDF is that they are exact and always contain the true accuracy the desired fraction of the time.\\n\\nLet\'s compare this with the commonly used bound obtained by approximating the binomial PMF with a normal distribution. The motivation for the normal approximation comes from the central limit theorem, which states that for a binomial distribution with parameters $N$ and $p$, the distribution of the empirical accuracy, $\\\\hat{p} = k/N$, converges to a normal distribution as the sample size, $N$, goes to infinity,\\n$$\\n\\\\hat{p} \\\\stackrel{d}{\\\\longrightarrow} \\\\mathcal{N}\\\\left(p, \\\\frac{p(1-p)}{N}\\\\right).\\n$$\\n\\nThis motivates the use of the traditional two-standard deviation confidence interval in which one reports\\n$$\\n\\\\Pr\\\\left\\\\{ | p - \\\\hat{p} | \\\\le 1.96 \\\\,\\\\hat{\\\\sigma} \\\\right\\\\} \\\\ge 95\\\\%\\n~ ~ ~ \\\\text{where} ~ ~ ~\\n\\\\hat{\\\\sigma} = \\\\sqrt{\\n  \\\\frac{ \\\\hat{p}(1-\\\\hat{p}) }{N}\\n}.\\n$$\\n\\nBut it\'s well known that the normal distribution poorly approximates the sampling distribution of $\\\\hat{p}$ when $p$ is close to zero or one. For instance, if we observe zero errors on the test set, then $\\\\hat{p}$ will equal 1.0 (i.e., 100% empirical accuracy), and the sample standard deviation, $\\\\hat{\\\\sigma}$, will equal zero. The estimated lower bound will therefore be equal to the empirical accuracy of 100%, which is clearly unbelievable.\\n\\nAnd since we train classifiers to have as close to 100% accuracy as possible, the regime in which $p$ is close to one is of major interest. Thus, exact confidence intervals based on the binomial CDF are both more accurate and practically useful than those based on the normal approximation.\\n\\n\\n## Conclusion\\n\\nAt Groundlight, we\'ve put a lot of thought and effort into assessing the performance of our customers\' ML models so they can easily understand how their detectors are performing. This includes the use of balanced accuracy as the summary performance metric and exact confidence intervals to convey the precision of the reported metric.\\n\\nHere we\'ve provided a detailed tour of the methods we use to estimate confidence intervals around balanced accuracy. The estimated intervals are exact in that they possess the stated coverage, no matter how many ground truth labeled examples are available for testing. Our aim in this post has been to provide a better understanding of the metrics we display, how to interpret them, and how they\'re derived. We hope we\'ve succeeded! If you are interested in reading more about these topics, see the references and brief appendices below.\\n\\n\\n## References\\n\\n[\\\\[Langford, 2005\\\\]](https://jmlr.org/papers/v6/langford05a.html) *Tutorial on Practical Prediction Theory for Classification*. Journal of Machine Learning Research 6 (2005) 273\u2013306.\\n\\n[\\\\[Brodersen et al., 2010\\\\]](https://ieeexplore.ieee.org/document/5597285) *The balanced accuracy and its posterior distribution*. Proceedings of the 20th International Conference on Pattern Recognition, 3121-24.\\n\\n\\n### Appendix A - the union bound\\n\\nRecall that the union bound states that for a collection of events, $A_1, A_2, \\\\ldots, A_n$, the probability that at least one of them occurs is less than the sum of the probabilities that each of them occurs:\\n$$\\\\Pr\\\\left\\\\{ \\\\cup_{i=1}^n A_i \\\\right\\\\} \\\\le \\\\sum_{i=1}^n \\\\Pr(A_i).$$\\n\\nPictorially, the union bound is understood from the image below which shows that area of the union of the regions is no greater than the sum of the regions\' areas.\\n\\n<figure>\\n  <img src={require(\'./images/binomial-tails/union-bound.png\').default} width=\\"350px\\" />\\n  <figcaption>\\n  <small>\\n    <b>Figure 8:</b> Visualizing the union bound. The area of each region $A_i$ corresponds to the probability that event $A_i$ occurs.\\n    The sum of the total covered area must be less than the sum of the individual areas.\\n  </small>\\n  </figcaption>\\n</figure>\\n\\n### Appendix B - interpretation of confidence intervals\\n\\nThe semantics around frequentist confidence intervals is subtle and confusing. The construction of a 95% upper confidence bound does **NOT** imply there is a 95% probability that the true accuracy is less than the bound. It only guarantees that the true accuracy is less than the upper bound in at least 95% of the cases that we run the the upper confidence bounding procedure (assuming we run the procedure many many times). For each individual case, however, the true accuracy is either greater than or less than the bound. And thus, for each case, the probability that the true accuracy is less than the bound equals either 0 or 1, we just don\'t know which.\\n\\nIf you instead desire more conditional semantics, you need to use Bayesian credible intervals. See [Brodersen et al., 2010](https://ieeexplore.ieee.org/document/5597285) for a nice derivation of credible intervals for balanced accuracy."},{"id":"raspberry-pi-computer-vision","metadata":{"permalink":"/python-sdk/blog/raspberry-pi-computer-vision","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2024-01-02-groundlight-pi-gen.md","source":"@site/blog/2024-01-02-groundlight-pi-gen.md","title":"Linux OS Images for Computer Vision on Raspberry Pi","description":"Groundlight simplifies the setup process by providing ready-to-use OS images for Raspberry Pi","date":"2024-01-02T00:00:00.000Z","formattedDate":"January 2, 2024","tags":[{"label":"raspberry-pi","permalink":"/python-sdk/blog/tags/raspberry-pi"},{"label":"mns","permalink":"/python-sdk/blog/tags/mns"}],"readingTime":4.84,"hasTruncateMarker":true,"authors":[{"name":"Blaise Munyampirwa","title":"Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg"},{"name":"Leo Dirac","title":"CTO and Co-founder at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png","imageURL":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png"}],"frontMatter":{"title":"Linux OS Images for Computer Vision on Raspberry Pi","description":"Groundlight simplifies the setup process by providing ready-to-use OS images for Raspberry Pi","slug":"raspberry-pi-computer-vision","authors":[{"name":"Blaise Munyampirwa","title":"Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg"},{"name":"Leo Dirac","title":"CTO and Co-founder at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png","imageURL":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png"}],"tags":["raspberry-pi","mns"],"image":"/img/gl-icon400.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Tales from the Binomial Tail: Confidence intervals for balanced accuracy","permalink":"/python-sdk/blog/confidence-intervals-for-balanced-accuracy"},"nextItem":{"title":"Best practices for best results with Groundlight","permalink":"/python-sdk/blog/best-practices"}},"content":"Happy New Year everybody!  If you got a fancy new Raspberry Pi 5 for Christmas, you might be wondering what to do with it.  Well, we have a suggestion:  build a computer vision application with it!  And we have all the tools you need to get started.\\n\\nRaspberry Pi offers a great platform for computer vision (CV), ranging from home hobby projects to serious industrial applications. However, setting up a Raspberry Pi for computer vision can be a time-consuming process. [Groundlight Pi-Gen](https://github.com/groundlight/groundlight-pi-gen), simplifies the setup process by providing ready-to-use OS images for Raspberry Pi.\\n\\n\x3c!-- truncate --\x3e\\n\\n(Note that here, when we say \\"image\\" we mean an OS image, which is a file containing a snapshot of an operating system - linux - that can be installed onto a new machine.  These are not photos or pictures, which are also of course important in computer vision.  Oh jargon...)\\n\\n## Raspberry Pi OS Images pre-built with Computer Vision Software\\nTo download a Linux image for your Raspberry Pi, loaded with all the software you need for computer vision, \\ngo to the [releases](https://github.com/groundlight/groundlight-pi-gen/releases) section in Groundlight Pi-Gen to find Raspberry Pi OS images (`.img.xz` files) that have pre-configured software environments for computer vision. These images are ready to be flashed onto a Raspberry Pi.\\n\\nThese include a fast, modern version of python (3.11), along with key libraries like [OpenCV](https://opencv.org/) for classic algorithms and device management, [Numpy](https://numpy.org/) for fast math, [FrameGrab](https://code.groundlight.ai/python-sdk/blog/introducing-framegrab) for declarative access to image sources, and of course [Groundlight](https://pypi.org/project/groundlight/) for fully-managed visual understanding models.  We\'ve set up a `venv` for you to avoid the dreaded \\"externally-managed-environment\\" error which plagues many newer python versions, while still letting you use good-old `pip` to add more.  (We like `poetry` and `conda`, and these will also work fine if you prefer them.)\\n\\nThere are several flavors of OS image available.  The smaller ones are suitable for headless use, while the larger ones include a desktop GUI with a browser.  The key differences are the size of the download and the amount of time it takes to flash the image onto a microSD card.  The [available flavors in the current release](https://github.com/groundlight/groundlight-pi-gen/releases) are:\\n\\n![Comparison of Groundlight Pi-Gen OS image flavors](./images/2024-01-02-groundlight-pi-gen/download-assets.png \\"Comparison of Groundlight Pi-Gen OS image flavors\\")\\n\\n- `desktop`: Image with Groundlight MNS and a desktop GUI with a browser.  Appropriate for a Raspberry Pi with a screen attached.\\n- `mns-headless`: Image with Groundlight Monitoring Notification Server (MNS) for headless use.\\n- `sdk-only`: Minimal image with the Python SDK and core libraries.  Suitable for headless use on smaller Raspberry Pi models such as the Pi Zero.\\n\\nA couple more flavors you might be interested in: We\'re planning a [kiosk mode](https://github.com/groundlight/groundlight-pi-gen/issues/15) for the desktop image, so that you can run a Groundlight MNS instance on a Raspberry Pi with a screen attached, and have it automatically start up in a browser.  \\nAlso note that the `edge` version which will download and run the ML models locally is not yet supported on Raspberry Pi, because the edge models requires a CUDA GPU.\\n\\n## Flashing the OS Image onto a microSD Card\\n\\nOnce you have [downloaded your image file](https://github.com/groundlight/groundlight-pi-gen/releases), the next step is to flash it onto a microSD card.  To do this, \\ndownload the [Raspberry Pi Imager](https://www.raspberrypi.com/software/) software. \\n\\n![Raspberry Pi Imager home screen](./images/2024-01-02-groundlight-pi-gen/rpi-imager-1.png \\"Raspberry Pi Imager home screen\\")\\n\\nAfter selecting your hardware type under \\"Choose Device\\", click \\"Choose OS\\" and scroll to the bottom to \\"Use custom\\".  \\n\\n![Raspberry Pi Imager use custom OS](./images/2024-01-02-groundlight-pi-gen/rpi-imager-2.png \\"Raspberry Pi Imager use custom OS\\")\\n\\nThen select the `.img.xz` file you downloaded.\\n\\n![Raspberry Pi Imager pick OS file](./images/2024-01-02-groundlight-pi-gen/rpi-imager-3.png \\"Raspberry Pi Imager pick OS file\\")\\n\\nThen choose your microSD card with the \\"Choose Storage\\" button, and then click \\"Next\\".  \\nYou\'ll get a prompt asking \\"Use OS customization?\\" which is optional, but very cool.  Choose \\"Edit settings\\", and you\\ncan set your Wi-Fi credentials, enable SSH login with a public key.\\n\\n![Rasterberry Pi Imager OS customization](./images/2024-01-02-groundlight-pi-gen/rpi-imager-4.png \\"Rasterberry Pi Imager OS customization\\")\\n\\nWhen you\'re done configuring settings, click \\"Save\\" and then \\"Yes\\" to confirm.  Writing the image to the microSD card will take a few minutes.  When it\'s done, just pop the SD card into your pi, and power it up!  If it all works properly, you\'ll be able to access your Raspberry Pi over the network without needing to plug in a keyboard, mouse, or monitor.  (We like to plug it into Ethernet for the first boot, because we find that the Raspberry Pi\'s Wi-Fi can be a bit finicky, even if properly configured.)\\n\\n\\n### No-code machine vision with Monitoring Notification Server (MNS)\\nIf you opted to install the `desktop` or `mns-headless` image, you\'ll have a web application called the [Groundlight Monitoring Notification Server (MNS)](https://github.com/groundlight/monitoring-notification-server),\\nwhich is a web application that allows you set up a computer vision pipeline without writing any code, and have it notify you when it detects something of interest.\\n\\nAfter setting up your Raspberry Pi with Groundlight OS, wait a few minutes for it to finish downloading everything, and then access the MNS by navigating to `http://[your-raspberry-pi\'s-IP-address]:3000` in a web browser, or if you\'re running the desktop version, open [`http://localhost:3000/`](http://localhost:3000).  \\n\\n![MNS sample home screen](./images/2024-01-02-groundlight-pi-gen/mns-home.png \\"MNS sample home screen\\")\\n\\nIt will prompt you for your [Groundlight API token](docs/getting-started/api-tokens), which you can get with a free account at [app.groundlight.ai](https://app.groundlight.ai).  Then you can describe your visual query in natural language, and how you want the MNS to notify you when it detects something of interest.  For best-practices on how to describe your visual query, see [this blog post](https://code.groundlight.ai/python-sdk/blog/best-practices).\\n\\n## Get Started for Free\\nTo start building your own computer vision solutions, sign up for a free account at [app.groundlight.ai](https://app.groundlight.ai). Dive into Groundlight Pi-Gen for a hassle-free introduction to AI-powered computer vision on Raspberry Pi.\\n\\nIf you have any questions, please reach out to us on the in-application chat at [app.groundlight.ai](https://app.groundlight.ai) or on [GitHub](https://github.com/groundlight/python-sdk/issues)."},{"id":"best-practices","metadata":{"permalink":"/python-sdk/blog/best-practices","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2023-12-15-best-practices.md","source":"@site/blog/2023-12-15-best-practices.md","title":"Best practices for best results with Groundlight","description":"How to get the best chance of success from Groundlight detectors","date":"2023-12-15T00:00:00.000Z","formattedDate":"December 15, 2023","tags":[{"label":"how-to","permalink":"/python-sdk/blog/tags/how-to"},{"label":"best-practices","permalink":"/python-sdk/blog/tags/best-practices"}],"readingTime":4.385,"hasTruncateMarker":true,"authors":[{"name":"Paulina Varshavskaya","title":"Head of R&D at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/932933bc26/varshap.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/932933bc26/varshap.jpg"},{"name":"Sunil Kumar","title":"ML Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg"},{"name":"Blake Thorne","title":"Head of Marketing at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/daf4a78ec3/thorneb.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/daf4a78ec3/thorneb.jpg"}],"frontMatter":{"title":"Best practices for best results with Groundlight","description":"How to get the best chance of success from Groundlight detectors","slug":"best-practices","authors":[{"name":"Paulina Varshavskaya","title":"Head of R&D at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/932933bc26/varshap.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/932933bc26/varshap.jpg"},{"name":"Sunil Kumar","title":"ML Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/a265e322bd/kumars.jpg"},{"name":"Blake Thorne","title":"Head of Marketing at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/daf4a78ec3/thorneb.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/daf4a78ec3/thorneb.jpg"}],"tags":["how-to","best-practices"],"image":"/img/gl-icon400.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Linux OS Images for Computer Vision on Raspberry Pi","permalink":"/python-sdk/blog/raspberry-pi-computer-vision"},"nextItem":{"title":"Introducing Groundlight\'s FrameGrab Library","permalink":"/python-sdk/blog/introducing-framegrab"}},"content":"Want to get the best chance of success from your new Groundlight detectors? Here are five suggestions from the Groundlight science team that can help you get the best performance possible. \\n\\nCome at it from the point of view of making answering your image query question as easy as possible. \\nPretend you\u2019re explaining the task to a novice. What would you need to do to set them up for success?\\n\\n\x3c!-- truncate --\x3e\\n\\n## Phrase the question right\\nYou will have more success asking questions that can in principle be answered by a reasonable person \\nsimply by looking at a single image from the detector. \\n\\n:white_check_mark: **DO:** \\"Is there a part staged in front of the robot ready for picking up?\\"   \\n:x: **DON\'T:** \\"Am I awesome?\\"\\n\\nThink about how you will use the output of your detector, so the Yes and No answers align with your expectations. A technically correct answer to a vague question may be of no use to you. For example, if you have a camera pointing down on a kitchen range and would like to get an alert if there\'s a fire, phrase the query so that normal gas burner flames are excluded. \\n\\n:white_check_mark: **DO:** \\"Is there a fire in the pan? (Ignore normal gas burner flames)\\"  \\n:x: **DON\'T:** \\"Is there a fire?\\"\\n\\n## Put details in the notes\\nIs any specialized knowledge required to answer your query? \\nUse the notes dialog to provide explanations of any assumed knowledge or definitions of technical terms. \\nLike in the fire extinguisher example above, consider adding short definitions inside the text of the query. \\n \\n:white_check_mark: **DO:** \u201cIs the fiducial (etched arrow on the gear surface) aligned with the painted chain link?\u201d  \\n:white_check_mark: **DO:** \u201cIs the fence fully closed? (Metal bar on the fence must be touching the plywood wall)\u201d\\n\\nHere\u2019s an example of detailed notes for a detector asking \u201cIs there a streetcar visible? READ NOTES\u201d: \\n\\n[![Screenshot of detailed notes for a detector](./images/2023-12-15-best-practices/streetcar_visible_notes.png \\"Detailed notes for a detector asking \\\\\\"Is there a streetcar visible? READ NOTES\\\\\\"\\")](./images/2023-12-15-best-practices/streetcar_visible_notes.png)\\n\\nIn this case, the customer even drew on the example images to point out where the street car might appear. \\nDetailed notes may be especially useful if the question is about a smaller region of the scene in the image.\\n\\n## Think of edge cases\\nHow do you want to treat unclear or edge cases?\\nSometimes it\u2019s impossible to answer the question based on the image, for example, when it\u2019s too dark \\nat night to tell, or the view is temporarily obstructed by something moving in front of the camera. \\nDo you know how you\u2019d like to treat those cases? \\n\\n:white_check_mark: **DO:** Add notes like \u201cIf the image is too dark to tell, the answer should be YES.\u201d\\n\\nIn the fire extinguisher example below, the customer wrote \u201cIf you can\u2019t see the fire extinguisher, \\nit is blocked\u201d inside the query text, drawing attention to the most important potential edge case.\\n\\n[![Screenshot for a detector with edge case in query](./images/2023-12-15-best-practices/fire_extinguisher_blocked_yes.png \\"A detector with a detailed query including a likely potential edge case (fire extinguisher not visible).\\")](./images/2023-12-15-best-practices/fire_extinguisher_blocked_yes.png)\\n\\nDetailed notes on foreseeable edge cases will prevent confusion by backend labelers and result in \\nquicker learning for your detector at less cost to you. \\n\\n## Seed with a few examples\\nIt helps to add a few labels yourself early on, in order to provide good examples for backend labelers and the \\nnew ML model. For best results, if you have example images for both YES and NO answers, send \\nthem through early on, and add the corresponding labels. Having at least 2 customer \u201cground truth\u201d \\nanswers for each class of Yes or No will also give you ML performance metrics on your detector.\\n\\n![Blue button before 2 examples of each class are provided](./images/2023-12-15-best-practices/label_button_before.png \\"\\")\\n\\n## Only you know the truth\\nCheck periodically under the Flagged tab on your detector\'s detail page to see if any images may still be confusing. Click on the \\"Override Label\\" button to provide the correct answer in those cases. \\n\\n[![Screenshot of image flagged as needing better examples](./images/2023-12-15-best-practices/flagged_images.png \\"Partial screenshot of a Flagged view\\")](./images/2023-12-15-best-practices/flagged_images.png)\\n\\nIt\'s also good practice to continue adding a few ground truth labels here and there by clicking on the \u201cKeep labeling\u201d button \\non the detector details page, in order to get tighter confidence bounds on your detector\u2019s performance metrics.\\n\\n---\\n> :mortar_board: *Read an in-depth discussion of how we assess detector accuracy and the confidence bounds reported for your detector\'s performance:*\\n[Tales from the Binomial Tail: Confidence intervals for balanced accuracy](2024-01-16-binomial-tails.md)\\n---\\n\\nIf you notice labeling mistakes, correct them by providing your own answer. Then consider adding \\nextra instructions in the notes. You can upload screenshots or images inside the notes dialog too. \\nOur labeling staff will be notified whenever you make changes to your notes so they stay up to date with how you want your detector to behave and can quickly address misconceptions.\\n\\n## [Ready to start?](https://app.groundlight.ai)\\n\\nWe hope these tips give you a great start. If you haven\u2019t already, you can sign up for a free account at https://app.groundlight.ai. Dive into [Groundlight Pi-Gen](https://github.com/groundlight/groundlight-pi-gen) for a hassle-free introduction to AI-powered computer vision on Raspberry Pi.\\n\\nIf you have any questions, please reach out to us on the in-application chat or via email to support@groundlight.ai."},{"id":"introducing-framegrab","metadata":{"permalink":"/python-sdk/blog/introducing-framegrab","editUrl":"https://github.com/groundlight/python-sdk/tree/main/docs/blog/blog/2023-12-06-framegrab.md","source":"@site/blog/2023-12-06-framegrab.md","title":"Introducing Groundlight\'s FrameGrab Library","description":"We would like to introduce you to the FrameGrab library.","date":"2023-12-06T00:00:00.000Z","formattedDate":"December 6, 2023","tags":[{"label":"groundlight-extensions","permalink":"/python-sdk/blog/tags/groundlight-extensions"},{"label":"framegrab","permalink":"/python-sdk/blog/tags/framegrab"}],"readingTime":3.12,"hasTruncateMarker":true,"authors":[{"name":"Tim Huff","title":"Engineering intern at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/06b25bf1a6/hufft.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/06b25bf1a6/hufft.jpg"},{"name":"Blaise Munyampirwa","title":"Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg"},{"name":"Leo Dirac","title":"CTO and Co-founder at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png","imageURL":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png"},{"name":"Tyler Romero","title":"Senior ML Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/368053d79a/romerot.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/368053d79a/romerot.jpg"},{"name":"Michael Vogelsong","title":"Chief ML Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/c87b9d30f4/vogelsongm.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/c87b9d30f4/vogelsongm.jpg"}],"frontMatter":{"title":"Introducing Groundlight\'s FrameGrab Library","description":"We would like to introduce you to the FrameGrab library.","slug":"introducing-framegrab","authors":[{"name":"Tim Huff","title":"Engineering intern at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/06b25bf1a6/hufft.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/06b25bf1a6/hufft.jpg"},{"name":"Blaise Munyampirwa","title":"Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/d12109465d/munyampirwab.jpg"},{"name":"Leo Dirac","title":"CTO and Co-founder at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png","imageURL":"https://a-us.storyblok.com/f/1015187/284x281/602a9c95c5/diracl.png"},{"name":"Tyler Romero","title":"Senior ML Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/368053d79a/romerot.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/368053d79a/romerot.jpg"},{"name":"Michael Vogelsong","title":"Chief ML Engineer at Groundlight","image_url":"https://a-us.storyblok.com/f/1015187/1000x1000/c87b9d30f4/vogelsongm.jpg","imageURL":"https://a-us.storyblok.com/f/1015187/1000x1000/c87b9d30f4/vogelsongm.jpg"}],"tags":["groundlight-extensions","framegrab"],"image":"/img/gl-icon400.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Best practices for best results with Groundlight","permalink":"/python-sdk/blog/best-practices"}},"content":"At Groundlight, we continue to build infrastructure that allows our customers to easily use computer \\nvision without a pre-existing dataset for industrial inspection, retail analytics, mobile robotics, and \\nmuch more. We\'ve built many features towards the goal of declarative computer vision, and today we are excited to \\nintroduce FrameGrab, a Python library designed to make it easy to grab frames from\\ncameras or streams. \\n\\nFrameGrab supports generic USB cameras, RTSP streams, Basler USB cameras, Basler GigE cameras, and Intel RealSense depth cameras. \\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Grabbing Camera Frames\\n\\nFrame grabber objects are configured through YAML. The configuration combines the camera type, camera ID, and the camera\\noptions. The YAML config contains many configurable features, but only `input_type` is required. Valid choices for \\n`input_type` include \\n\\n* generic_usb\\n* rtsp\\n* realsense\\n* basler \\n\\nHere is an example of how to use the generic USB configuration \\n\\n```python notest\\nfrom framegrab import FrameGrabber \\n\\nconfig = \\"\\"\\"\\nname: Front Door Camera\\ninput_type: generic_usb\\nid:\\n  serial_number: 23432570\\noptions:\\n    resolution:\\n        height: 1080\\n        width: 1920\\n    zoom:\\n        digital: 1.5\\n\\"\\"\\"\\n\\ngrabber = FrameGrabber.create_grabber_yaml(config)\\nframe = grabber.grab()\\n\\n# Do real work with the frame \\n\\n# Finally release the grabber object \\ngrabber.release()\\n\\n```\\n\\nFor the full set of configurable parameters, please refer to the [FrameGrab repository](https://github.com/groundlight/framegrab/tree/main).\\n\\n## Multi-cam Configuration \\n\\nIf you have multiple cameras of the same type plugged in, we recommend you include serial numbers in the YAML config to \\nensure proper pairing. The default pairing behavior is sequential (i.e., configurations will be paired with cameras in \\na sequential ordering). \\n\\nYou can add serial numbers for multiple cameras like this\\n\\n```yaml \\nGL_CAMERAS: |\\n  - name: on robot arm\\n    input_type: realsense\\n    options: \\n      depth:\\n        side_by_side: 1\\n      crop:\\n        relative:\\n          right: .8\\n  - name: conference room\\n      input_type: rtsp\\n      id: \\n        rtsp_url: rtsp://admin:password@192.168.1.20/cam/realmonitor?channel=1&subtype=0\\n      options:\\n        crop:\\n          pixels:\\n            top: 350\\n            bottom: 1100\\n            left: 1100\\n            right: 2000\\n  - name: workshop\\n    input_type: generic_usb\\n    id:\\n      serial_number: B77D3A8F\\n\\n```\\n\\n## FrameGrab Autodiscovery Mode \\n\\nAmong other features, FrameGrab also includes autodiscovery mode. This allows you to automatically connect to all cameras \\nthat are plugged into your machine (or discoverable on the network). Autodiscovery will load up default configurations \\nfor each camera. \\n\\n:::note\\n\\nPlease note that RTSP streams cannot be autodiscovered in this manner. RTSP URLs must be pre-specified in the \\nconfigurations. \\n\\n:::\\n\\nWe recommend autodiscovery for simple applications where you don\'t need to set any special options on your cameras. \\nIt is also a convenient method for finding the serial numbers of your cameras in case they are not printed on them. \\n\\nBelow is a short example of how to launch autodiscovery mode. \\n\\n```python notest\\nfrom framegrab import FrameGrabber\\n\\ngrabbers = FrameGrabber.autodiscover()\\n\\n# Print some information about the discovered cameras\\nfor grabber in grabbers.values():\\n    print(grabber.config)\\n\\n    # Do real work \\n\\n    # Release the frame grabber object \\n    grabber.release()\\n\\n```\\n\\n\\n## Using FrameGrab for Motion Detection \\n\\nWith this release, we also continue to support [motion detection](https://en.wikipedia.org/wiki/Motion_detection) via frame differencing, a \\nfast algorithm for easily detecting motion in a sequence of frames. \\n\\nTo use motion detection, initialize the MotionDetector instance with the desired percentage of pixels \\nneeded to change in an image for it to be flagged for motion and the minimum brightness change for each pixel for it \\nto be considered changed. Here is a comprehensive example. \\n\\n```python notest\\nfrom framegrab import FrameGrabber, MotionDetector\\n\\nconfig = {\\n    \'input_type\': \'webcam\',\\n}\\ngrabber = FrameGrabber.create_grabber(config)\\nmotion_detector = MotionDetector(pct_threshold=motion_threshold, val_threshold=60)\\n\\nwhile True:\\n    frame = grabber.grab()\\n    if frame is None:\\n        print(\\"No frame captured!\\")\\n        continue\\n\\n    if motion_detector.motion_detected(frame):\\n        print(\\"Motion detected!\\")\\n\\n```\\n\\n\\n## Conclusion \\n\\nRecent releases of FrameGrab add various easy to use features. We now support \\nmultiple camera types and continue to support motion detection. \\n\\nIf you encounter any issues while using FrameGrab, please feel free to file an issue in our [GitHub repository](https://github.com/groundlight/framegrab)\\nand while there, review guidelines for [contributing](https://github.com/groundlight/framegrab#contributing) to this library."}]}')}}]);